{
  "version": "1.13.0",
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Insight Opportunity Schema",
  "description": "Defines the structure for documenting big opportunities in the INSIGHT phase. Opportunities capture product/market possibilities discovered through research, analysis, competitive intelligence, and user feedback before they become structured strategies.",
  "$comment": "Insight Opportunities are how EPF captures product possibilities during exploratory research before committing to roadmap development. They document: (1) What opportunity exists (title, description, context), (2) Why it matters (evidence, value hypothesis), (3) How we'll know if it's real (success indicators), (4) Current validation status (identified → validated → roadmap or invalidated → archived). Use Insight Opportunities to: Evaluate multiple possibilities before choosing strategic direction (diverge before converging), Build evidence base before resource commitment (research-backed decisions), Track opportunity lifecycle from identification through validation to roadmap inclusion or rejection, Maintain 'opportunity backlog' of ideas not yet ready for roadmap (defer = not now, but keep watching). Opportunities differ from Roadmap Recipes: Opportunities are pre-strategy (what might we do?), Roadmaps are strategic commitment (what will we do?). Opportunities differ from Feature Ideas: Opportunities are market-level (who needs what?), Features are solution-level (how do we build it?). Write Insight Opportunities during INSIGHT phase before READY phase, update validation status as evidence accumulates, promote validated opportunities to Roadmap Recipes in READY phase. Typical opportunity lifecycle: Cycle 1-2 'identified' with growing evidence, Cycle 3-4 'validated' or 'invalidated' based on data, Cycle 5+ validated opportunity becomes roadmap or deferred opportunity stays in backlog.",
  "type": "object",
  "properties": {
    "opportunity": {
      "type": "object",
      "description": "The opportunity being documented, including all context, evidence, and validation status. This is the core container for opportunity data.",
      "$comment": "The opportunity object encapsulates everything needed to evaluate a product/market possibility: identification data (who/what/why), evidence (quantitative/qualitative research), value assessment (user + business + strategic), and validation tracking (status/confidence/date). Good opportunity documentation is: (1) Evidence-based (cites research, data, competitive analysis), (2) Value-focused (clear user and business benefits), (3) Testable (defines success indicators), (4) Trackable (status and confidence updates over time), (5) Actionable (sufficient detail to decide 'build roadmap' or 'defer/invalidate'). Use this structure to maintain a portfolio of opportunities: Some at 'identified' status (early research), some 'validated' (ready for roadmap), some 'invalidated' (learned it's not viable), some 'deferred' (good idea, wrong time). Review opportunity portfolio regularly (quarterly suggested) to identify patterns, update evidence, adjust confidence, promote validated opportunities to roadmaps.",
      "properties": {
        "id": {
          "type": "string",
          "description": "Unique identifier for the opportunity (e.g., 'opp-enterprise-collaboration', 'opp-mobile-first-users', 'opp-api-ecosystem'). Use descriptive IDs that capture the essence of the opportunity.",
          "$comment": "Opportunity IDs create the tracking lineage from initial identification through validation to roadmap inclusion (or rejection). Use stable, descriptive IDs that reveal what the opportunity is about: 'opp-enterprise-workspace' is better than 'opp-001'. Format: 'opp-' prefix + brief description (lowercase, hyphens). These IDs persist across documents: an opportunity identified in INSIGHT phase might be referenced in Assessment Reports when validating assumptions, mentioned in Calibration Memos when deciding strategic direction, and eventually become the basis for a Roadmap Recipe ID if validated and prioritized. Track opportunity lineage: 'opp-mobile-first-users' (INSIGHT) → validated → 'roadmap-mobile-experience' (READY/FIRE) shows how research converts to strategy. Stable IDs enable querying: 'Show me all opportunities related to enterprise' or 'What happened to opportunity X?'",
          "minLength": 10,
          "maxLength": 80,
          "pattern": "^opp-[a-z0-9-]+$"
        },
        "title": {
          "type": "string",
          "description": "Clear, compelling title for the opportunity that communicates the core value proposition or market possibility. Should be understandable to both technical and business stakeholders.",
          "$comment": "Title is your opportunity's elevator pitch - it should communicate what makes this worth investigating in 5-10 words. Good titles are: (1) Value-focused, not feature-focused ('Enable remote team collaboration' better than 'Add video calling'), (2) Audience-specific ('Enterprise workspace management' better than 'Better organization'), (3) Outcome-oriented ('Reduce time-to-insight for analysts' better than 'Improve dashboard'), (4) Memorable and distinctive (easy to reference in discussions). Title patterns: User segment + core benefit ('Mobile users: offline-first experience'), Problem + solution space ('Solve multi-tenant data isolation challenges'), Market category ('PLG motion for enterprise adoption'). Example titles: 'Enable non-technical users to build automations', 'API-first platform for ecosystem partners', 'Real-time collaboration for distributed teams', 'Self-service analytics for business users', 'Mobile-first experience for field workers'. Avoid: Generic titles ('Improve user experience'), Technology-first titles ('Implement GraphQL subscriptions'), Inside-baseball titles only engineers understand.",
          "minLength": 10,
          "maxLength": 100
        },
        "description": {
          "type": "string",
          "description": "Detailed description of the opportunity explaining what it is, why it matters, and what success looks like. This is the narrative that builds conviction.",
          "$comment": "Description expands on the title with full context, painting a picture of the opportunity space. Good descriptions follow a pattern: (1) What: Describe the opportunity space clearly (market, user segment, problem domain), (2) Why: Explain why this matters now (market timing, user pain, competitive dynamics, strategic positioning), (3) Who: Identify target users/customers (segments, personas, use cases), (4) Evidence: Reference key data points (even if full evidence is in evidence section), (5) Success vision: Paint picture of what success looks like (user outcomes, business metrics, market position). Structure description as narrative, not bullet points. Use concrete examples. Reference specific users, use cases, or market data. Example description: 'Enterprise customers (50+ employees) struggle with workspace management across multiple teams and projects. Current tools force tradeoffs between flexibility (Notion, spreadsheets) and structure (enterprise PM tools). Research with 30 enterprise customers reveals this is a top-3 pain point, costing 5-10 hours/week per manager. Market size: 50K potential customers, $200M TAM. Success looks like: Customers consolidate 3-5 tools into our platform, managers save 5+ hours/week, NPS >40 in enterprise segment, expansion revenue from team → company adoption.' Aim for 200-500 words (100-300 for smaller opportunities, 300-800 for strategic initiatives).",
          "minLength": 200,
          "maxLength": 1500
        },
        "context": {
          "type": "object",
          "description": "Market and user context for the opportunity, defining who needs this, what problems they face, how big the opportunity is, and why it matters now.",
          "$comment": "Context provides the strategic framing that helps evaluate opportunity priority and fit. It answers: Who is this for? (target_segment), What problems do they have? (pain_points), How big is the prize? (market_size), Why now? (urgency). Good context documentation: (1) Specific target segment (not 'users' but 'enterprise IT administrators' or 'mobile field workers'), (2) Prioritized pain points (most acute first), (3) Evidence-backed market sizing (research-based, not guesses), (4) Urgency reasoning (market timing, competitive pressure, user demand). Use context to filter opportunities: Opportunities targeting high-priority segments with acute pain points and large market size get validated first. Opportunities with low urgency get deferred. Review context quarterly as markets evolve.",
          "properties": {
            "target_segment": {
              "type": "string",
              "description": "Specific user or customer segment this opportunity targets. Be precise about who needs this solution (e.g., 'Enterprise IT administrators managing 100+ SaaS apps', 'Mobile field service technicians', 'Data analysts in financial services').",
              "$comment": "Target segment defines who you're solving for, which determines product requirements, go-to-market strategy, and success metrics. Good segments are: (1) Specific and actionable (not 'users' but 'product managers at B2B SaaS companies'), (2) Measurable (can estimate size and reach), (3) Accessible (know how to find and sell to them), (4) Value-aligned (their needs match your capabilities/strategy). Segment by: Job role and seniority ('VP of Sales', 'junior data analyst'), Company characteristics ('Series B SaaS startups', 'Fortune 500 manufacturing'), Use case or workflow ('real-time analytics for trading desks', 'field service dispatching'), Technical sophistication ('non-technical business users', 'ML engineers'). Avoid: Generic segments ('businesses', 'teams'), Too broad segments ('all developers'), Demographic-only segments without behavioral/needs context. Example segments: 'Engineering managers at 50-500 person tech companies managing distributed teams', 'Marketing operations specialists at B2C companies managing 10+ martech tools', 'Financial analysts at hedge funds performing real-time market analysis', 'Product designers at agencies juggling 5+ client projects simultaneously'. Reference your North Star's target user personas where applicable - opportunities should align with strategic focus on specific user segments.",
              "minLength": 20,
              "maxLength": 200
            },
            "pain_points": {
              "type": "array",
              "description": "Key problems or frustrations this opportunity addresses, ordered by severity or frequency. Each pain point should be specific and evidence-backed.",
              "$comment": "Pain points are the 'pull' that makes an opportunity compelling - the stronger the pain, the more users will adopt your solution. Good pain points: (1) Specific and observable ('Manually copying data between 3 systems takes 2 hours daily'), not vague ('Hard to collaborate'), (2) Quantified when possible ('Costs $50K annually per team', '5-10 hours wasted per week'), (3) Evidence-backed (cite user research, support tickets, or market data), (4) Prioritized (most severe/frequent first). Document pain point patterns: Frequency (how often does this hurt?), Severity (how much does it cost/hurt when it happens?), Workarounds (what do users do today? how painful is the workaround?), Consequences (what happens if pain persists? churn risk? competitive loss?). Structure each pain point as: Problem statement + Evidence + Impact. Example pain points: 'Teams spend 5-10 hours weekly in status meetings because no single source of truth exists (research: 30 user interviews, avg 7.2 hours reported)', 'Data silos force manual reconciliation between systems, creating 15-20% error rate in financial reports (customer support: #1 escalation reason)', 'Mobile app crashes during offline use, blocking 40% of field technicians' workflows (analytics: 2,500 crash reports/month)', 'Onboarding new team members takes 2 weeks due to undocumented tribal knowledge (HR data: avg 9.5 days to productivity)'. Capture 2-5 pain points; more than 7 suggests opportunity too broad (split into multiple opportunities), fewer than 2 suggests weak value prop (validate pain exists).",
              "minItems": 1,
              "maxItems": 8,
              "items": {
                "type": "string",
                "minLength": 30,
                "maxLength": 300
              }
            },
            "market_size": {
              "type": "string",
              "description": "Estimated size of the addressable market for this opportunity. Include TAM (Total Addressable Market), SAM (Serviceable Addressable Market), or revenue potential. Cite sources and assumptions.",
              "$comment": "Market size quantifies the opportunity's ceiling - how big could this be if fully successful? Good market sizing: (1) Uses multiple methods (top-down from market research, bottom-up from user counts × price), (2) Cites credible sources (industry reports, analyst firms, competitive data), (3) Shows assumptions transparently (conversion rates, pricing tiers, adoption curves), (4) Provides realistic ranges (best/base/worst case or confidence intervals). Market size components: TAM (Total Addressable Market): Total market revenue if you captured 100% (usually unrealistic but shows ceiling). Example: '$5B enterprise collaboration software market (Gartner 2024)'. SAM (Serviceable Addressable Market): Portion you can realistically reach with your product/GTM. Example: '$500M subset targeting 50-500 person tech companies'. SOM (Serviceable Obtainable Market): What you can capture in 3-5 years given competition. Example: '$50M if we achieve 10% market share'. Revenue potential: Users × Price × Adoption. Example: '50K target enterprises × $10K ACV × 20% adoption = $100M potential'. Include timeframe and key assumptions. Example market sizing: 'TAM: $2B API management market (Forrester 2024). SAM: $400M subset focused on high-scale use cases (>10M API calls/month). Target: 5K enterprises globally, $50K ACV, achieve 500 customers (10% penetration) = $25M ARR in 3 years. Assumptions: 30% market CAGR, competitive displacement rate 15%/year, sales cycle 6 months.' If market size unknown, state that explicitly and outline research plan: 'Market size TBD - need to survey 50 target enterprises to estimate prevalence and willingness-to-pay.' Avoid: Unrealistic TAM claims ($100B markets), Uncited numbers, Confusion between users and revenue.",
              "minLength": 50,
              "maxLength": 500
            },
            "urgency": {
              "type": "string",
              "description": "Why this opportunity matters now - market timing, competitive dynamics, user demand intensity, or strategic windows. What changes if we wait 6-12 months?",
              "$comment": "Urgency explains why this opportunity should be validated/pursued now versus later - the 'why now' that drives prioritization. Good urgency rationale: (1) Market timing (regulatory changes, technology shifts, adoption curves), (2) Competitive dynamics (competitor moves, market consolidation, windows closing), (3) User demand intensity (escalating pain, vocal requests, churn risk), (4) Strategic positioning (ecosystem shifts, platform opportunities, first-mover advantages), (5) Dependency sequencing (required for other opportunities, technical debt resolution). Urgency patterns: Market shifts: 'New GDPR requirements create need for data governance tools - enterprises must comply by Q3', 'Remote work adoption accelerated by pandemic created 3x demand for collaboration tools'. Competitive pressure: 'Two competitors launched mobile apps this quarter - our customers requesting parity', 'Market consolidating around API-first architectures - need to be in that category'. User escalation: 'Enterprise segment churn rate increased from 5% to 12% due to missing SSO/SCIM - top support issue', '#1 feature request for 8 consecutive months with 500+ votes'. Strategic windows: 'Platform play requires ecosystem before competitors lock in developers', 'Sales team can't close enterprise deals without this - blocking $2M pipeline'. Technology enablers: 'LLM capabilities now make this feasible where it wasn't 12 months ago', 'New database technology reduces cost/complexity by 10x'. Document both opportunity cost (what we miss if we wait) and risk (what happens if we don't act). Example urgency: 'Competitors launching AI features in Q2/Q3 - risk being seen as legacy product if we don't have similar capabilities. Additionally, LLM costs decreased 80% this year making previously uneconomical features now viable. Customer research shows 45% would switch to competitor with AI features (n=200 survey). Window: Next 6 months before market leaders establish AI category positions.' If urgency is low, state that explicitly - some opportunities are 'good ideas, wrong time' and should be deferred: 'Low urgency - no immediate competitive threat, users satisfied with workarounds, focus on higher-priority opportunities first. Revisit in 6 months.'",
              "minLength": 50,
              "maxLength": 500
            }
          },
          "required": ["target_segment", "pain_points"]
        },
        "evidence": {
          "type": "object",
          "description": "Evidence backing the opportunity hypothesis - quantitative data, qualitative insights, and competitive analysis. Strong evidence increases confidence in opportunity validation.",
          "$comment": "Evidence transforms opportunities from opinions into testable hypotheses. Good evidence: (1) Multiple sources (triangulate from analytics, research, competitive intel), (2) Specific and verifiable (cite sources, provide context), (3) Representative (not cherry-picked outliers), (4) Recent (market/user needs evolve). Evidence quality determines validation priority: High-evidence opportunities (multiple sources, clear patterns, recent data) → validate fast. Low-evidence opportunities (hunches, anecdotes, old data) → gather evidence first or defer. Document evidence limitations and gaps transparently. Track evidence strength over time as you gather more data during validation.",
          "properties": {
            "quantitative": {
              "type": "array",
              "description": "Data-driven evidence from analytics, metrics, surveys, or market research. Each item includes source and key insight.",
              "$comment": "Quantitative evidence provides measurable, objective data about opportunity viability. Sources: Product analytics (usage data, conversion rates, retention metrics), User surveys (feature requests, satisfaction scores, willingness-to-pay), Market research (TAM/SAM sizing, growth rates, adoption curves), Sales data (pipeline size, win/loss reasons, deal velocity), Support data (ticket volume, resolution time, escalation rates), Financial data (revenue impact, cost savings, ROI projections). Good quantitative evidence: (1) Cites specific sources (Google Analytics showing 40% users hitting free tier limits), (2) Provides context (Usage 3x industry benchmark per Mixpanel 2024 report), (3) Shows trends (Support tickets about X increased 150% quarter-over-quarter), (4) Quantifies impact ($500K ARR opportunity based on 100 customers at $5K ACV). Structure each item: Source + Metric + Context + Insight. Examples: Amplitude analytics Q4 2024 showing 12K monthly active users attempted premium feature but hit paywall, with 40% abandoning session (vs 8% baseline) indicating strong unmet demand for premium tier; Customer survey (n=200, Nov 2024) finding 67% would pay $50/mo for SSO/SCIM features and 45% saying lack of SSO blocks expansion, validating enterprise opportunity; Salesforce pipeline data showing $2M in enterprise deals stalled due to missing compliance features, with 8 opportunities worth $250K+ each; Zendesk support showing 340 tickets about mobile offline functionality in Q3 (up from 120 in Q2), indicating escalating pain point. Avoid: Vague claims, uncited numbers, single data points without context, outliers presented as trends. Aim for 3-5 quantitative evidence items; more shows strong validation case, fewer suggests need for more research.",
              "minItems": 0,
              "maxItems": 10,
              "items": {
                "type": "object",
                "properties": {
                  "source": {
                    "type": "string",
                    "description": "Where this data came from - analytics tool, survey, market report, sales system, support system, financial analysis, etc. Be specific.",
                    "minLength": 10,
                    "maxLength": 150
                  },
                  "insight": {
                    "type": "string",
                    "description": "Key finding from this data - what does it tell us about the opportunity? Include metrics, context, and implications.",
                    "minLength": 30,
                    "maxLength": 400
                  }
                },
                "required": ["source", "insight"]
              }
            },
            "qualitative": {
              "type": "array",
              "description": "Insight-driven evidence from user research, interviews, field studies, or case studies. Each item includes source and key insight.",
              "$comment": "Qualitative evidence provides deep understanding of user needs, behaviors, and motivations that quantitative data cannot capture. Sources: User interviews (1-on-1 conversations, discovery calls), Customer feedback (emails, support conversations, sales calls), Usability studies (observation, task completion, think-aloud protocols), Field research (shadowing users, contextual inquiry), Case studies (detailed examples of user workflows), Community insights (forum posts, social media, user groups). Good qualitative evidence: (1) Representative (patterns across multiple users, not single outliers), (2) Contextual (captures why and how, not just what), (3) Specific (quotes, examples, observed behaviors), (4) Recent (within 6 months unless timeless insight). Structure each item: Source + Sample size/context + Key finding + Representative quote. Examples: User interviews (n=15 enterprise IT admins, Oct-Nov 2024) found all manually export data to Excel for compliance reports due to lack of native reporting, taking 2-4 hours weekly; Customer success calls (8 accounts, Q3 2024) revealed mobile users frequently lose work when connectivity drops, with common workaround of noting work on paper then re-entering later; Sales discovery (12 enterprise deals, Sept-Dec 2024) showed security/compliance teams block adoption due to missing SSO and audit logs; Support conversation analysis (200 tickets, Q4 2024) found users confused by terminology mismatch between product and their workflow, using different words for same concepts. Avoid: Single anecdotes, vague summaries, unverified claims, old insights (over 12 months without revalidation). Aim for 3-5 qualitative evidence items showing patterns across multiple users/conversations.",
              "minItems": 0,
              "maxItems": 10,
              "items": {
                "type": "object",
                "properties": {
                  "source": {
                    "type": "string",
                    "description": "Where this insight came from - user interviews, customer feedback, field research, usability study, etc. Include sample size and timeframe.",
                    "minLength": 10,
                    "maxLength": 150
                  },
                  "insight": {
                    "type": "string",
                    "description": "Key finding from this research - patterns observed, user needs uncovered, behavioral insights, motivations, pain points, or workarounds. Include representative quotes or examples where helpful.",
                    "minLength": 30,
                    "maxLength": 400
                  }
                },
                "required": ["source", "insight"]
              }
            },
            "competitive_landscape": {
              "type": "array",
              "description": "Competitive intelligence about how others are addressing this opportunity - competitor moves, market gaps, differentiation opportunities, strategic positioning.",
              "$comment": "Competitive landscape reveals market validation (others solving this = real problem) and differentiation opportunities (gaps to exploit). Capture: Competitor moves (who solving what, how, their traction), Market gaps (problems competitors do not address well), Differentiation vectors (where you can win: features, pricing, target segment, go-to-market), Strategic positioning (category creation vs market entry). Good competitive analysis: (1) Specific (name competitors, cite their features/pricing/positioning), (2) Evidence-based (links to product, analyst reports, customer comparisons), (3) Gap-focused (what missing that we could uniquely provide), (4) Dynamic (acknowledge market is moving, competitors iterating). Competitive insight patterns: Direct competition - Competitor X launched mobile app Q3 2024 with offline support ($20/mo premium tier), validates demand but reviews cite sync issues (3.2 star avg), opportunity to do offline better. Market gaps - Top 5 competitors focus on enterprise ($100K+ ACVs) but mid-market ($10-50K) underserved per G2 reviews, 47% of reviews mention too expensive for their size. Category adjacency - CRM tools all have basic task management but none integrate project management well, users cobbling together 3-4 tools (Salesforce + Asana + Slack), integration opportunity. Emerging trends - AI features becoming table stakes, 3 competitors added AI assistants in 2024, analyst reports (Gartner) predict AI will be in 80% of products by 2025, must have AI to stay competitive. Customer insights - Win/loss analysis (Q4 2024): Lost 12 deals to Competitor Y due to SSO/SCIM; won 8 deals vs Competitor Z on better mobile experience, validates mobile as differentiator. Structure: Competitor/trend + Their approach + Gap/opportunity + Evidence. Avoid: Vague claims, uncited assertions, ignoring competitive strengths, static view. Aim for 2-5 competitive insights covering direct competitors, market gaps, and strategic positioning.",
              "minItems": 0,
              "maxItems": 8,
              "items": {
                "type": "string",
                "minLength": 50,
                "maxLength": 400
              }
            }
          }
        },
        "value_hypothesis": {
          "type": "object",
          "description": "Hypothesized value this opportunity creates - what it means for users, for the business, and for strategic positioning. This is your testable prediction of impact.",
          "$comment": "Value hypothesis is your bet on outcomes - what happens if we pursue this opportunity? Good value hypotheses: (1) Specific and measurable (not vague benefits but concrete outcomes), (2) Testable during validation (can you prove or disprove?), (3) Multi-dimensional (user + business + strategic perspectives), (4) Risk-aware (acknowledge assumptions and uncertainty). Structure: User value (jobs-to-be-done, pain relief, gains), Business value (revenue, retention, efficiency, cost reduction), Strategic fit (positioning, competitive advantage, ecosystem effects). Validate hypotheses progressively: INSIGHT phase documents initial hypothesis based on evidence → NAVIGATE phase refines hypothesis through experiments → FIRE phase delivers value → RETROSPECT phase validates actual vs predicted value. Track hypothesis evolution: When evidence changes, update hypothesis. When validation contradicts hypothesis, pivot or kill opportunity. Strong value hypotheses attract resources: Clear user value → easier to sell internally. Clear business value → easier to prioritize. Clear strategic fit → easier to align with leadership.",
          "properties": {
            "user_value": {
              "type": "string",
              "description": "What value does this create for users? Focus on jobs-to-be-done, problems solved, capabilities enabled, or friction removed. Be specific about the outcome users will experience.",
              "$comment": "User value answers: What can users do that they could not do before? What pain goes away? What becomes faster, easier, cheaper, better? Good user value statements: (1) Outcome-focused (describes end state, not features: Save 5 hours weekly, not Has automation feature), (2) Measurable (quantify time saved, errors reduced, tasks completed), (3) Contextual (who benefits, when, why it matters), (4) Compelling (would users pay for this? switch products for this?). Structure: [User segment] can [achieve outcome] [measurably better than today] because [core capability]. Examples: Enterprise IT admins can provision new user accounts in 2 minutes instead of 30 minutes because automated SSO/SCIM sync eliminates manual data entry and reduces errors from 15% to near-zero. Mobile field technicians can complete work orders offline and auto-sync when connectivity returns, eliminating 3-4 hours weekly spent re-entering data from paper notes. Data analysts can generate compliance reports in 5 minutes instead of 2 hours because native export templates match auditor requirements, reducing manual Excel formatting and formula errors. Engineering managers can identify blocked work in real-time through automated dependency tracking, reducing sprint delays by 20% and improving predictability. Avoid: Feature descriptions (Has SSO), Vague benefits (Better collaboration), Unmeasurable claims (Much faster), Tech-focused outcomes (Uses GraphQL API). User value drives adoption: Strong user value → users seek it out. Weak user value → requires heavy promotion, training, incentives. No user value → users resist or abandon. Validate user value early: Can you demo the outcome in 30 seconds? Would users pay for just this? Does it solve top-3 pain point? Aim for 100-400 characters: Enough detail to visualize outcome, concise enough to remember and communicate.",
              "minLength": 100,
              "maxLength": 400
            },
            "business_value": {
              "type": "string",
              "description": "What value does this create for the business? Focus on revenue impact, cost savings, retention improvement, market positioning, or strategic advantage. Quantify when possible.",
              "$comment": "Business value answers: How does this help the company grow, save money, reduce risk, or compete better? Good business value statements: (1) Quantified (revenue numbers, cost savings, retention percentages, efficiency gains), (2) Timebound (impact within 6 months, annual recurring value), (3) Evidence-based (cite assumptions: 500 potential customers at $5K ACV), (4) Risk-adjusted (acknowledge uncertainty: assumes 30% conversion rate). Business value dimensions: Revenue (new customers, expansion revenue, price increases), Retention (reduce churn, increase engagement, improve satisfaction), Efficiency (reduce support costs, faster time-to-value, operational savings), Market position (competitive advantage, category leadership, ecosystem effects), Risk reduction (compliance, security, reliability improvements). Structure: [Financial/strategic outcome] of [quantified amount] [within timeframe] because [mechanism]. Examples: Generate $2M incremental ARR in 12 months by selling SSO/SCIM as $50/mo add-on to 200 existing enterprise customers (assumes 40% attach rate based on win/loss analysis). Reduce support costs by $150K annually through 30% ticket reduction as mobile offline capability eliminates sync-related issues (currently 340 tickets/quarter at $45 avg resolution cost). Increase enterprise deal close rate from 35% to 50% by removing compliance objection in 8 active opportunities worth $2M pipeline value (sales feedback: compliance is blocker in 60% of enterprise deals). Improve net retention from 95% to 105% by reducing churn in accounts requesting AI features (customer success data: 12 at-risk accounts, $400K ARR, citing competitor AI capabilities). Avoid: Vague impacts (Improves revenue), Uncited numbers (Could be millions), Feature-level metrics (10K API calls), Non-strategic outcomes (Looks cool). Business value drives prioritization: High business value + high user value → top priority. High business value + low user value → selling challenge. Low business value + high user value → lifestyle business. Connect business value to company goals: Revenue targets, margin improvements, market share objectives. Aim for 100-400 characters with specific numbers and assumptions.",
              "minLength": 100,
              "maxLength": 400
            },
            "strategic_fit": {
              "type": "string",
              "description": "How does this opportunity align with broader company strategy, competitive positioning, or long-term vision? Why does this matter beyond immediate user and business value?",
              "$comment": "Strategic fit answers: Does this move us toward our North Star? Does this strengthen our competitive position? Does this create platform effects or ecosystem value? Good strategic fit statements: (1) Vision-aligned (connects to company mission, market category, long-term positioning), (2) Differentiating (creates competitive moats: network effects, data advantages, switching costs, ecosystem lock-in), (3) Multiplicative (enables future opportunities, creates platform value, attracts ecosystem partners), (4) Defensive or offensive (protects market position or captures new territory). Strategic fit dimensions: Category leadership (defines or dominates a market category), Competitive moats (creates sustainable advantages), Platform effects (enables ecosystem growth, third-party value), Market expansion (new segments, geographies, use cases), Technology position (builds core IP, data assets, AI capabilities). Structure: This [strengthens/enables/protects] [strategic dimension] because [mechanism], [future implications]. Examples: Strengthens enterprise positioning by adding security/compliance capabilities that match competitors, enabling us to compete for $100K+ deals we currently lose; positions us as AI-native platform (vs legacy competitors adding AI as afterthought) as market shifts toward AI-first buying criteria per Gartner 2024 predictions. Enables platform strategy by creating third-party integration marketplace - attracts 50+ partners building on our API (competitor has 200+ partners, we have 8) - network effects drive stickiness and differentiation. Protects mobile-first positioning as competitors launch mobile apps - maintains differentiation we established 2 years ago that drives 40% of new customer acquisition; failure to invest risks commoditization. Creates data advantage through usage analytics that improve AI recommendations - more users → better recommendations → more engagement → more data → flywheel effect that competitors cannot easily replicate. Avoid: Generic claims (Aligns with strategy), Everything is strategic (be selective), Disconnected from evidence (cite competitors, trends, data), Short-term thinking (strategic = long-term compounding). Strategic fit influences sequencing: High strategic fit opportunities may justify investment despite lower immediate business value. Low strategic fit opportunities risk diluting focus even if financially attractive. Optional field (can be null): Not all opportunities have strong strategic implications. Opportunistic feature requests may deliver user and business value without strategic weight. Document honestly. Aim for 100-400 characters when present, connecting to specific strategic objectives.",
              "minLength": 50,
              "maxLength": 400
            }
          },
          "required": ["user_value", "business_value"]
        },
        "success_indicators": {
          "type": "array",
          "description": "How will we know if pursuing this opportunity was the right decision? Define measurable outcomes that indicate success during validation and after delivery.",
          "$comment": "Success indicators provide measurable validation criteria - what evidence proves (or disproves) that this opportunity delivers expected value? Good success indicators: (1) Measurable (specific metrics with numeric targets), (2) Timebound (achievable within validation or delivery timeframe), (3) Leading and lagging (mix of early signals and ultimate outcomes), (4) Aligned with value hypothesis (metrics that prove user value, business value, strategic fit claims). Types of success indicators: User behavior (adoption rate, engagement frequency, retention, NPS, task completion), Business outcomes (revenue, conversion, retention, cost reduction, efficiency), Validation signals (experiment results, prototype feedback, willingness-to-pay tests), Strategic markers (competitive wins, partner interest, market recognition). Structure each indicator: Metric name + Target value + Timeframe + Measurement method. Examples: Adoption rate reaches 40% of target segment within 60 days of launch, measured through product analytics tracking feature activation. Monthly active users for this capability exceeds 5,000 within 90 days, indicating sustained engagement beyond initial trial. Net Promoter Score for users of this feature averages 8+ (vs 6.5 product average), measured through in-app surveys, indicating strong value delivery. Conversion rate from free to paid increases by 10 percentage points (from 3% to 13%) among users who engage with this capability, measured through billing system. Enterprise deal close rate improves by 15 percentage points when this capability is demonstrated in sales process, tracked through Salesforce opportunity data. Support ticket volume decreases by 30% for issues related to this workflow, indicating friction removal and user success. Validation experiments: During INSIGHT/NAVIGATE phases, indicators help decide whether to proceed: If prototype testing shows 70%+ of users complete key task successfully → validate opportunity. If willingness-to-pay survey shows 50%+ would pay → validate business value. If experiment shows 20% adoption in 2 weeks → strong signal. Post-launch tracking: During FIRE/RETROSPECT phases, indicators measure actual vs predicted value: Compare actual metrics to targets. Adjust strategy if metrics lag. Celebrate if metrics exceed. Kill if metrics fail. Avoid: Vanity metrics (page views, signups without engagement), Output metrics (features shipped, not outcomes achieved), Unmeasurable claims (better satisfaction without measurement method), Single metric (need multiple perspectives). Aim for 3-6 indicators covering user behavior, business outcomes, and validation signals. Too many indicators scatter focus. Too few indicators miss important dimensions.",
          "minItems": 1,
          "maxItems": 8,
          "items": {
            "type": "object",
            "properties": {
              "metric": {
                "type": "string",
                "description": "Name of the metric being measured - what specific outcome or behavior indicates success? Be precise about what is being tracked.",
                "minLength": 10,
                "maxLength": 150
              },
              "target": {
                "type": "string",
                "description": "Target value for this metric - the specific, measurable outcome that indicates success. Include numeric targets, timeframes, and comparison baselines.",
                "minLength": 20,
                "maxLength": 300
              }
            },
            "required": ["metric", "target"]
          }
        },
        "status": {
          "type": "string",
          "description": "Current lifecycle status of this opportunity - where it sits in the validation/decision pipeline.",
          "$comment": "Status tracks opportunity through validation lifecycle. Identified: Opportunity documented based on evidence, hypothesis formed, awaiting validation decision. Core question: Is this worth investigating further? Typically INSIGHT phase output - initial research complete, hypothesis articulated, evidence gathered. Next step: Design validation experiments (NAVIGATE) or defer for later. Validated: Validation experiments confirmed opportunity is worth pursuing - evidence supports value hypothesis, confidence is high. Core question: Does data support our hypothesis? Requires validation experiments: user testing, prototype feedback, market validation, willingness-to-pay tests. Next step: Proceed to FIRE phase (build and deliver) or refine scope based on validation learnings. Invalidated: Validation experiments contradicted hypothesis - evidence shows opportunity is not viable, value claims are wrong, or market does not care. Core question: What did we learn and what do we do now? Requires explicit invalidation: experiment failed, market research negative, cost exceeds value. Next step: Kill opportunity or pivot hypothesis (rarely resurrect exactly as-is). Deferred: Opportunity is valid but not prioritized now - timing is wrong, dependencies missing, higher priorities exist. Core question: Why not now and when might we revisit? Requires clear deferral reason: market not ready, technology not mature, team capacity, strategic sequencing. Next step: Set review trigger (quarterly check-in, technology milestone, market signal). Status transitions: Identified → Validated (validation experiments succeed) or Invalidated (validation experiments fail) or Deferred (deprioritized). Validated → Delivered (FIRE phase complete, shipped to users). Invalidated → Archived (documented learnings, closed). Deferred → Identified (revisited later with fresh evidence). Track status changes: Record when and why status changed. Link to validation evidence. Document decision rationale. Use status for portfolio management: How many opportunities in each status? Are we validating fast enough? Are we killing rigorously? Status informs resource allocation: Identified opportunities need validation experiments. Validated opportunities need delivery teams. Invalidated opportunities need postmortems. Deferred opportunities need periodic review.",
          "enum": ["identified", "validated", "invalidated", "deferred"]
        },
        "confidence_level": {
          "type": "string",
          "description": "How confident are we in this opportunity hypothesis based on current evidence?",
          "$comment": "Confidence reflects evidence quality and validation completeness - how much do we trust this opportunity will deliver expected value? Low confidence: Hypothesis is interesting but speculative - limited evidence, untested assumptions, high uncertainty. Characteristics: Based on anecdotes or single data points. Market/user research is shallow. Value hypothesis is unvalidated. Competitive analysis is incomplete. Many unknowns remain. Typical for newly identified opportunities. Next steps: Gather more evidence, run experiments, validate assumptions. Do not commit large resources. Medium confidence: Hypothesis has supporting evidence but gaps remain - some validation done, directional signals positive, but not conclusive. Characteristics: Multiple evidence sources agree. Small-scale validation experiments succeeded. User research shows interest. Business case is plausible but not proven. Competitive position is understood. Some risks remain. Typical for opportunities after initial validation. Next steps: Address remaining unknowns, expand validation, prepare for delivery decision. High confidence: Hypothesis is well-validated - strong evidence, successful experiments, clear path to value. Characteristics: Comprehensive evidence from multiple sources. Validation experiments with representative users succeeded. Clear demand signals (willingness-to-pay, competitive wins, user requests). Business case is proven at small scale. Risks are understood and mitigated. Typical for opportunities ready for FIRE phase investment. Next steps: Commit resources, build, deliver, measure. Confidence evolves: Start low with initial hypothesis. Increase through evidence gathering and validation. Drops if new evidence contradicts hypothesis. Rises again with successful experiments. Confidence drives decisions: Low confidence → invest in validation, not delivery. Medium confidence → run experiments before committing. High confidence → green-light delivery with appropriate resources. Confidence is not certainty: Even high confidence carries risk. Acknowledge uncertainty. Plan for learning. Be willing to pivot if reality differs from hypothesis. Update confidence as you learn: When validation succeeds → increase confidence. When evidence contradicts hypothesis → decrease confidence. When new competitive intel emerges → reassess confidence.",
          "enum": ["low", "medium", "high"]
        },
        "validation_date": {
          "type": ["string", "null"],
          "format": "date",
          "description": "Date when this opportunity was validated (status changed to 'validated' or 'invalidated'). Null if not yet validated.",
          "$comment": "Validation date marks when opportunity moved from hypothesis to validated decision. Records when validation experiments completed and confidence reached decision threshold. Null for opportunities in identified or deferred status (validation not yet attempted or not relevant). Set to experiment completion date when status changes to validated or invalidated. Tracks validation velocity: How long from identified to validated? Are we validating quickly or slowly? Do high-value opportunities get validated faster? Use for portfolio metrics: How many opportunities validated this quarter? What is average time-to-validation? Are we bottlenecked on validation capacity? Date format is ISO 8601: YYYY-MM-DD (2024-11-15). Update when status changes: Set validation_date when status → validated or invalidated. Keep null for identified and deferred statuses. Do not backdate - use actual validation completion date."
        }
      },
      "required": ["id", "title", "description", "context", "value_hypothesis", "status"]
    }
  },
  "required": ["opportunity"]
}
