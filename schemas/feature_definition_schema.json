{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "feature_definition_schema.json",
  "title": "EPF Feature Definition Schema",
  "version": "2.0.0",
  "description": "Schema for validating feature definition files that bridge EPF strategy to implementation tools. Follows lean documentation principles - git handles versioning. Supports rich value narratives, design guidance, and architecture patterns. Version 2.0: Enhanced with prescriptive quality constraints (exactly 4 personas, 3-paragraph narratives, top-level scenarios, required context fields, rich dependencies).",
  "$comment": "This schema validates YAML feature definition files located in _instances/{product}/features/READY/. Feature definitions are the bridge between strategic EPF artifacts (North Star, Strategy Formula, Value Model, Roadmap Recipe) and implementation tools (OpenSpec, Linear, Cursor, etc.). They capture WHAT users need (job-to-be-done), WHY it matters (value propositions), HOW it works (capabilities, scenarios), and WHERE it fits (strategic context, dependencies). Enhanced validation ensures high-quality, tool-consumable feature definitions.",
  "type": "object",
  "required": ["id", "name", "slug", "status", "strategic_context", "definition"],
  "additionalProperties": false,
  "properties": {
    "id": {
      "type": "string",
      "pattern": "^fd-[0-9]+$",
      "description": "Unique identifier for the feature definition following format: fd-{number}",
      "$comment": "Feature Definition ID. Format: 'fd-' prefix + zero-padded sequential number (e.g., fd-001, fd-042, fd-123). Must be unique across all feature definitions in the product instance. Used for cross-references from scenarios (scn-*), capabilities (cap-*), contexts (ctx-*), and dependencies. Sequential numbering recommended for discoverability. Example: fd-001 (first feature), fd-015 (fifteenth feature).",
      "examples": ["fd-001", "fd-042", "fd-123"]
    },
    "name": {
      "type": "string",
      "minLength": 1,
      "description": "Human-readable feature name that clearly describes what the feature does",
      "$comment": "Feature name appears in UI, documentation, and tools. Should be concise (3-6 words) but descriptive. Use title case. Avoid jargon unless domain-specific. Think: 'What would a user call this?' Examples: 'Digital Twin Ecosystem', 'Compliance Document Processing', 'Real-Time Meeting Analytics'. This name will appear in Linear issues, OpenSpec titles, and navigation menus.",
      "examples": ["Digital Twin Ecosystem", "Compliance Document Processing", "Real-Time Meeting Analytics", "Automated Board Report Generation"]
    },
    "slug": {
      "type": "string",
      "pattern": "^[a-z0-9]+(-[a-z0-9]+)*$",
      "description": "URL-safe lowercase identifier derived from name, using hyphens instead of spaces",
      "$comment": "Slug is used in URLs, file paths, and API endpoints. Must be lowercase, alphanumeric, with hyphens separating words. No underscores, spaces, or special characters. Should match the name semantically. Used for: file naming (slug.yaml), URL routing (/features/slug), API paths (/api/features/slug). Example: name='Digital Twin Ecosystem' → slug='digital-twin-ecosystem'.",
      "examples": ["digital-twin-ecosystem", "compliance-document-processing", "real-time-meeting-analytics", "automated-board-report-generation"]
    },
    "status": {
      "type": "string",
      "enum": ["draft", "ready", "in-progress", "delivered"],
      "description": "Current lifecycle status of the feature definition",
      "$comment": "Feature lifecycle status affects tool behavior. 'draft' = incomplete definition, tools should not process. 'ready' = validated definition, tools can create implementation backlogs (Linear issues, OpenSpec specs, test skeletons). 'in-progress' = implementation started, tools track progress and validate coverage. 'delivered' = implementation complete, tools verify coverage and finalize references. Status changes trigger lifecycle hooks (see integration_specification.yaml Section 12).",
      "examples": ["draft", "ready", "in-progress", "delivered"]
    },
    "strategic_context": {
      "type": "object",
      "description": "Traceability links to strategic EPF artifacts (Value Model, Roadmap Recipe). Enables alignment verification and impact analysis.",
      "$comment": "Strategic context connects feature definitions to higher-level EPF strategy artifacts. This N:M mapping enables: (1) Value Model alignment - which L2/L3 value paths benefit from this feature, (2) Roadmap tracking - which track(s) this feature belongs to, (3) Assumption validation - which roadmap assumptions this feature helps test. Loose coupling by design - strategy can evolve independently from features. Tools can aggregate features by value path or track for portfolio views.",
      "required": ["contributes_to", "tracks"],
      "properties": {
        "contributes_to": {
          "type": "array",
          "items": {
            "type": "string",
            "pattern": "^(Product|Commercial|Strategy|OrgOps)\\.[A-Za-z]+\\.[A-Za-z]+",
            "description": "Value Model L2/L3 path that receives value from this feature. Format: {L1}.{L2}.{L3}"
          },
          "minItems": 1,
          "description": "Value Model paths (L2/L3) that receive value from this feature. Multiple paths indicate cross-cutting value delivery.",
          "$comment": "Value Model paths use hierarchical dot notation. Level 1 (L1) = Pillar (Product, Commercial, Strategy, OrgOps). Level 2 (L2) = Theme (e.g., Product.Operate, Commercial.Acquire). Level 3 (L3) = Capability (e.g., Product.Operate.Monitoring). A feature can contribute to multiple paths - this is normal for platform features. Example: A 'Dashboard' feature might contribute to both 'Product.Operate.Monitoring' and 'Product.Decide.Analytics'. Reference Value Model (value_model_schema.json) for valid paths.",
          "examples": [["Product.Operate.Monitoring"], ["Commercial.Acquire.Discovery", "Commercial.Expand.Retention"], ["Strategy.Align.Vision", "Strategy.Execute.Delivery"]]
        },
        "tracks": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": ["product", "strategy", "org_ops", "commercial"]
          },
          "minItems": 1,
          "description": "Roadmap track(s) this feature belongs to. Determines planning horizon and priority evaluation context.",
          "$comment": "Roadmap tracks organize features by strategic pillar. 'product' = user-facing features, 'commercial' = revenue/GTM features, 'strategy' = strategic initiatives, 'org_ops' = internal operations. A feature can belong to multiple tracks if it delivers cross-cutting value. Example: An 'API Platform' feature might be in both 'product' (customer API access) and 'strategy' (ecosystem play). Reference Roadmap Recipe (roadmap_recipe_schema.json) for track definitions.",
          "examples": [["product"], ["product", "commercial"], ["strategy", "org_ops"]]
        },
        "assumptions_tested": {
          "type": "array",
          "items": {
            "type": "string",
            "pattern": "^asm-(p|s|o|c)-[0-9]+$",
            "description": "Assumption ID from roadmap. Format: asm-{track_prefix}-{number} where track_prefix is p(roduct), s(trategy), o(rg_ops), or c(ommercial)"
          },
          "description": "Roadmap assumptions this feature helps validate through implementation and user feedback.",
          "$comment": "Assumption testing creates feedback loops from implementation to strategy. Format: 'asm-' prefix + track prefix (p/s/o/c) + number. Example: 'asm-p-042' = Product track assumption #42. When feature is delivered, tools can report back to EPF AIM phase whether assumptions held true. This enables evidence-based strategy refinement. Not all features test assumptions - this field is optional. Reference Roadmap Recipe for assumption definitions.",
          "examples": [["asm-p-001", "asm-p-007"], ["asm-c-023"], ["asm-s-015", "asm-o-008"]]
        }
      }
    },
    "definition": {
      "type": "object",
      "description": "Core feature definition: WHY users need it (job-to-be-done + value propositions), WHAT it does (capabilities), and HOW it works (solution approach + architecture patterns).",
      "$comment": "Definition section is the heart of the feature. It captures: (1) Job-to-be-done - the user need in situation/motivation/outcome format, (2) Solution approach - high-level HOW from user perspective, (3) Capabilities - discrete units of value (what the feature DOES), (4) Value propositions - rich narratives showing transformation (WHY it matters), (5) Architecture patterns - key technical innovations. This section is heavily consumed by implementation tools (OpenSpec, Linear, test generators).",
      "required": ["job_to_be_done", "solution_approach", "capabilities"],
      "properties": {
        "job_to_be_done": {
          "type": "string",
          "minLength": 10,
          "description": "The user need this feature addresses, written in standardized format: When [situation], I want to [motivation], so I can [expected outcome].",
          "$comment": "Job-to-be-done (JTBD) captures user intent independent of solution. Format is prescriptive: 'When [situation], I want to [motivation], so I can [expected outcome].' Situation = context/trigger, Motivation = desired action, Outcome = value delivered. Example: 'When I receive a new compliance document, I want to automatically extract key entities and obligations, so I can quickly assess impact without manual review.' This format forces clarity and testability. Tools can parse JTBD to understand acceptance criteria. Multiple scenarios can share same JTBD category for grouping.",
          "examples": [
            "When I receive a new compliance document, I want to automatically extract key entities and obligations, so I can quickly assess impact without manual review.",
            "When planning quarterly board meetings, I want to see all pending agenda items and required materials in one view, so I can ensure nothing is missed.",
            "When our product strategy changes, I want to see which active features are affected, so I can re-prioritize implementation work."
          ]
        },
        "solution_approach": {
          "type": "string",
          "minLength": 10,
          "description": "High-level explanation of HOW this feature works from the user's perspective. Focus on user-visible behavior, not technical implementation.",
          "$comment": "Solution approach describes the user experience at a high level. Focus on: What does the user see? What actions do they take? What feedback do they get? Avoid technical jargon unless user-facing. Should be understandable by product stakeholders and designers. Example: 'Users upload compliance documents through a drag-and-drop interface. The system processes documents asynchronously and shows extraction progress. Once complete, users review extracted entities with confidence scores and can correct any errors before saving.' Length: 2-5 sentences typically.",
          "examples": [
            "Users upload compliance documents through a drag-and-drop interface. The system processes documents asynchronously using AI-powered entity extraction. Users review extracted entities with confidence scores and can correct errors before saving to the knowledge base.",
            "The dashboard presents a unified view of all board meeting materials, automatically pulling from connected calendars, document repositories, and task trackers. Users can drag-and-drop to reorder agenda items and see completion status in real-time.",
            "A visual dependency graph shows relationships between features, highlighting which ones depend on changed strategy elements. Users can filter by impact level and export affected feature lists for planning."
          ]
        },
        "capabilities": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["id", "name", "description"],
            "properties": {
              "id": {
                "type": "string",
                "pattern": "^cap-[0-9]+$",
                "description": "Unique capability identifier. Format: cap-{number}",
                "$comment": "Capability ID must be unique within the feature (not globally). Format: 'cap-' prefix + zero-padded number. Example: cap-001, cap-042. Capabilities are discrete units of value that the feature provides. They map to implementation specs (OpenSpec documents), Linear issues, and test suites. Tools use capability IDs for: (1) Coverage tracking - which capabilities have specs/tests, (2) Status reporting - which are implemented/delivered, (3) Cross-references - scenarios reference capabilities they exercise. Numbering should be sequential for discoverability.",
                "examples": ["cap-001", "cap-015", "cap-042"]
              },
              "name": {
                "type": "string",
                "description": "Short, action-oriented capability name. Use verb-noun format.",
                "$comment": "Capability names appear in UI, Linear issues, OpenSpec titles. Should be concise (2-4 words) and action-oriented. Use present tense verbs. Format: {Verb} {Object}. Good examples: 'Extract Entities', 'Validate Confidence', 'Export Report'. Bad examples: 'Entity Extraction Component', 'The ability to validate', 'Exporting'. Think: 'What specific thing can users DO with this capability?'",
                "examples": ["Extract Entities", "Validate Confidence Scores", "Export Report", "Schedule Notification", "Track Progress"]
              },
              "description": {
                "type": "string",
                "description": "Detailed explanation of what this capability does and what value it provides. Include key behaviors and constraints.",
                "$comment": "Capability description is consumed by spec-driven tools to generate implementation details. Include: (1) What it does (action), (2) What data it works with (inputs/outputs), (3) Key behaviors (validation, error handling), (4) Constraints (limits, requirements). Length: 2-4 sentences. Example: 'Extract structured entities (person names, dates, obligations, regulatory references) from uploaded compliance documents using AI-powered NLP. Assigns confidence scores (0-100%) to each extracted entity. Handles PDFs, Word docs, and scanned images. Requires document to be under 50 pages.' This level of detail enables OpenSpec to generate API specs, test generators to create test cases, etc.",
                "examples": [
                  "Extract structured entities (person names, dates, obligations, regulatory references) from compliance documents using AI-powered NLP. Assigns confidence scores (0-100%) to each entity. Handles PDFs, Word docs, and scanned images up to 50 pages.",
                  "Validate extracted entities by comparing against known databases and regulatory references. Flags potential errors and suggests corrections. Users can accept, reject, or modify suggested validations.",
                  "Generate compliance reports in multiple formats (PDF, Excel, HTML) with customizable templates. Includes extracted entities, confidence scores, validation results, and audit trail."
                ]
              }
            }
          },
          "minItems": 1,
          "description": "Key capabilities this feature provides. Each capability is a discrete unit of value that can be independently specified, implemented, and tested.",
          "$comment": "Capabilities are the building blocks of features. They represent distinct, testable units of functionality. Good capabilities are: (1) Atomic - can't be meaningfully subdivided, (2) Testable - clear success criteria, (3) Valuable - users would notice if missing, (4) Independent - minimal coupling to other capabilities, (5) Estimable - can be sized for planning. Number of capabilities varies: simple features may have 2-3, complex features may have 10-15. Too few = monolithic, hard to track. Too many = fragmented, overhead. Tools use capabilities for: work breakdown (Linear issues), spec generation (OpenSpec docs), test planning (test suites), coverage tracking (implementation_references)."
        },
        "personas": {
          "type": "array",
          "minItems": 4,
          "maxItems": 4,
          "items": {
            "type": "object",
            "required": ["id", "name", "role", "description", "goals", "pain_points", "usage_context", "technical_proficiency", "current_situation", "transformation_moment", "emotional_resolution"],
            "properties": {
              "id": {
                "type": "string",
                "pattern": "^[a-z]+(-[a-z]+)*$",
                "description": "Unique identifier for this persona using lowercase kebab-case (e.g., 'power-user', 'business-analyst').",
                "examples": ["power-user", "business-analyst", "system-admin", "new-user"]
              },
              "name": {
                "type": "string",
                "description": "Display name for the persona (e.g., 'Power User', 'Business Analyst').",
                "examples": ["Power User", "Business Analyst", "System Administrator", "New User"]
              },
              "role": {
                "type": "string",
                "description": "The primary job role or title for this persona (e.g., 'Senior Data Engineer', 'Product Manager').",
                "examples": ["Senior Data Engineer", "Product Manager", "IT Administrator", "Junior Analyst"]
              },
              "description": {
                "type": "string",
                "minLength": 30,
                "description": "Brief description of the persona's characteristics, expertise level, and typical context (30+ chars minimum, 50+ recommended for rich descriptions).",
                "examples": [
                  "Experienced professional with deep technical knowledge who uses advanced features daily and often customizes workflows.",
                  "Non-technical user focused on business outcomes who needs intuitive interfaces and clear guidance."
                ]
              },
              "goals": {
                "type": "array",
                "minItems": 2,
                "items": {
                  "type": "string",
                  "minLength": 10
                },
                "description": "List of primary goals this persona wants to achieve using the feature (2+ goals, each 10+ chars minimum, 20+ recommended).",
                "examples": [
                  ["Maximize productivity through automation", "Gain deep insights from data", "Share expertise with team"],
                  ["Complete tasks quickly without technical complexity", "Meet business deadlines", "Maintain work-life balance"]
                ]
              },
              "pain_points": {
                "type": "array",
                "minItems": 2,
                "items": {
                  "type": "string",
                  "minLength": 10
                },
                "description": "List of frustrations and challenges this persona currently faces (2+ pain points, each 10+ chars minimum, 20+ recommended).",
                "examples": [
                  ["Repetitive manual tasks consume hours daily", "Lack of visibility into system state", "Difficult to troubleshoot issues"],
                  ["Steep learning curve for new tools", "Fear of making mistakes", "Unclear next steps"]
                ]
              },
              "usage_context": {
                "type": "string",
                "minLength": 10,
                "description": "How and when this persona interacts with the feature, including frequency and typical scenarios (10+ chars recommended, 50+ for rich descriptions).",
                "examples": [
                  "Daily intensive use, expects advanced features",
                  "Uses feature multiple times daily as core part of workflow. Frequently explores advanced capabilities and integrations.",
                  "Uses feature occasionally when business needs arise. Prefers guided workflows and relies on defaults."
                ]
              },
              "technical_proficiency": {
                "type": "string",
                "enum": ["basic", "intermediate", "advanced", "expert"],
                "description": "The persona's technical skill level: basic (minimal technical knowledge), intermediate (comfortable with common tools), advanced (deep technical expertise), expert (master-level skills)."
              },
              "current_situation": {
                "type": "string",
                "minLength": 200,
                "description": "Rich narrative paragraph (200+ chars) describing the persona's current frustrations and pain points from their perspective. MUST include: specific scenarios, time/cost metrics, emotional context.",
                "$comment": "Rich narrative from this persona's perspective showing their current pain. Creates empathy and enables persona-specific customization of onboarding, documentation, and feature prioritization.",
                "examples": [
                  "As a power user managing complex data pipelines, I'm frustrated by the lack of advanced filtering options. I spend 2-3 hours daily manually sorting through results that should be automated. My team depends on my expertise, but I'm constantly context-switching between tools instead of focusing on high-value analysis work. The current workflow feels like fighting the system rather than being empowered by it."
                ]
              },
              "transformation_moment": {
                "type": "string",
                "minLength": 200,
                "description": "Rich narrative paragraph (200+ chars) describing the first concrete moment this persona experiences value from the feature. MUST include: specific action they take, immediate benefit they see, measurable improvement.",
                "$comment": "Rich narrative tailored to this persona's workflow showing the 'aha!' moment when the feature delivers value. Shows the specific experience that converts a skeptic into an advocate.",
                "examples": [
                  "When I first used the advanced query builder, I immediately saw my complex filtering needs represented visually. What previously required 2-3 hours of manual work was done in 5 minutes with precise controls over every parameter. The system anticipated my expert-level requirements and delivered exactly what I needed without compromise."
                ]
              },
              "emotional_resolution": {
                "type": "string",
                "minLength": 200,
                "description": "Rich narrative paragraph (200+ chars) describing the long-term emotional transformation and sustained value for this persona. MUST include: emotional state change, new capabilities, strategic outcomes.",
                "$comment": "Rich narrative focused on this persona's lasting transformation. Captures not just efficiency gains but how their work, confidence, and impact have fundamentally changed.",
                "examples": [
                  "Three months later, I've reclaimed 40+ hours monthly that I now invest in strategic initiatives and mentoring junior team members. The anxiety of manual data wrangling has been replaced by confidence in automated workflows. My team looks to me not just for technical execution but for strategic data insights, and I finally have the time and mental energy to deliver that value."
                ]
              }
            }
          },
          "description": "Exactly 4 detailed persona profiles representing different user types, experience levels, and use cases. Each persona includes comprehensive characteristics, goals, pain points, and rich narrative transformation (current_situation → transformation_moment → emotional_resolution).",
          "$comment": "QUALITY GUIDANCE: While personas is not formally required by schema structure, the validate-feature-quality.sh script enforces exactly 4 personas as a quality standard. This ensures features are designed for diverse user needs rather than a single user archetype. Each persona should represent a distinct technical proficiency level (basic, intermediate, advanced, expert) and use case. The 11 required fields per persona include: (1) 8 base fields defining persona characteristics (id, name, role, description, goals, pain_points, usage_context, technical_proficiency), (2) 3 narrative fields with 200+ char rich stories (current_situation, transformation_moment, emotional_resolution). Total persona section: 800-2000+ chars per persona, 3200-8000+ chars total. Personas enable: UX personalization, documentation targeting, feature prioritization by user type, empathy-driven design decisions."
        },
        "architecture_patterns": {
          "type": "array",
          "items": {
            "type": "object",
            "required": ["name", "description"],
            "properties": {
              "name": {
                "type": "string",
                "description": "Name of the architectural pattern. Use established pattern names when applicable.",
                "$comment": "Architecture pattern names should be recognizable to technical audiences. Prefer established patterns when applicable: 'Event Sourcing', 'CQRS', 'Saga Pattern', 'Service Mesh', 'API Gateway', 'Backend for Frontend (BFF)', 'Hexagonal Architecture', 'Repository Pattern'. For novel patterns, use descriptive names that convey intent: 'Attachable Services', 'Dynamic Policy Engine', 'Streaming Pipeline'. Avoid generic terms like 'Microservices' without specificity. Pattern names appear in: OpenSpec documents (architecture sections), ADRs (architecture decision records), onboarding docs. Good: 'Event-Driven Entity Extraction'. Bad: 'Our Architecture'.",
                "examples": ["Event Sourcing", "Attachable Services", "CQRS with Projections", "Saga Orchestration", "Dynamic Policy Engine", "Streaming ETL Pipeline"]
              },
              "description": {
                "type": "string",
                "minLength": 50,
                "description": "Explanation of the pattern, why it's used, and what problem it solves. Include key benefits and trade-offs.",
                "$comment": "Pattern description should explain: (1) What it is - high-level concept in 1 sentence, (2) Why we use it - what problem does it solve, what does it enable, (3) Key characteristics - what makes this pattern distinct, (4) Trade-offs if significant - complexity vs benefits. Length: 2-4 sentences (50-200 chars). Focus on value, not implementation details. Example: 'Event Sourcing pattern stores all state changes as events rather than current state. Enables full audit history, temporal queries, and event replay for debugging. Used for compliance tracking where regulatory requirements demand complete transaction history. Trade-off: increased storage and query complexity for current state.' Tools use this for: architecture documentation, onboarding materials, OpenSpec ADR sections.",
                "examples": [
                  "Event Sourcing stores all state changes as immutable events rather than current state. Enables complete audit history, temporal queries, and event replay. Used for compliance tracking where regulations demand full transaction history. Trade-off: increased storage and query complexity.",
                  "Attachable Services pattern allows features to be dynamically attached/detached from core entities without changing entity code. Enables rapid feature development and per-customer feature toggles. Used when feature set varies significantly by customer segment. Trade-off: service discovery overhead.",
                  "CQRS separates read and write models, optimizing each independently. Write model enforces business rules; read model is denormalized for query performance. Used when read patterns differ significantly from write patterns. Trade-off: eventual consistency and dual-model maintenance."
                ]
              },
              "components": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Key components or services that implement this pattern. Use concrete component names.",
                "$comment": "Components list identifies the specific parts of the system that embody this pattern. Use actual component/service names from your architecture, not generic terms. Good examples: 'DocumentProcessorService', 'EventStore', 'ProjectionBuilder', 'ComplianceAggregator'. Bad examples: 'The processor', 'Database', 'Backend'. Each component should be a recognizable unit that engineers can locate in codebase. If pattern spans many components, list the most significant 3-5. Tools use this for: architecture diagrams, dependency analysis, test planning (integration test boundaries). Format: array of service/component names, capitalized appropriately.",
                "examples": [
                  ["EventStore", "EventPublisher", "ProjectionBuilder", "SnapshotManager"],
                  ["DocumentProcessorService", "EntityExtractionEngine", "ConfidenceScorer", "ResultAggregator"],
                  ["CommandHandler", "QueryService", "EventBus", "ReadModelProjector"]
                ]
              }
            }
          },
          "description": "Core architectural innovations or patterns that define how this feature works technically. Focus on significant patterns that affect multiple components or represent key design decisions.",
          "$comment": "Architecture patterns capture the 'HOW it's built' from a technical architecture perspective. Include patterns that: (1) Span multiple components/services, (2) Represent significant design decisions, (3) Affect extensibility/scalability/maintainability, (4) Would need explanation for new engineers. Don't list every pattern - focus on the 2-4 most important. Simple CRUD features might have 0-1 patterns. Complex distributed features might have 3-5. Tools consume this for: architecture documentation generation (Mermaid diagrams, C4 models), ADR creation, onboarding documentation, OpenSpec architecture sections. Engineers reference patterns to understand system-level decisions, not component-level implementation."
        }
      }
    },
    "implementation": {
      "type": "object",
      "description": "Implementation guidance for spec-driven tools to consume",
      "properties": {
        "design_guidance": {
          "type": "object",
          "description": "UX philosophy and design principles guiding this feature's implementation. Provides design teams with constraints and inspiration.",
          "$comment": "Design guidance connects feature definition to design execution. It captures: (1) Principles - non-negotiable UX rules for this feature, (2) Inspirations - products/patterns to reference for design language, (3) Interaction patterns - specific UI behaviors to use. This section is consumed by: Design teams for visual design, Frontend engineers for implementation, OpenSpec for UI specification generation, Product for consistency checks. Unlike architecture_patterns (technical), design_guidance is user-facing and behavior-focused.",
          "properties": {
            "principles": {
              "type": "array",
              "items": {
                "type": "string"
              },
              "description": "Core design principles that must guide all design decisions for this feature. Use established UX principles when applicable.",
              "$comment": "Design principles are guardrails, not requirements. They help designers make consistent choices. Use established principles when they fit: 'Progressive Disclosure' (reveal complexity gradually), 'Principle of Least Astonishment' (behavior matches expectations), 'Immediate Feedback' (actions show results instantly), 'Discoverability' (features are findable without training), 'Forgiveness' (easy to undo mistakes), 'Consistency' (similar actions work similarly). Custom principles are fine if specific to feature context. Example: 'Always show confidence scores - users must trust AI-extracted data'. Length: 1 sentence per principle. Number: 2-5 principles typically. Tools use this for: design critique checklists, PR review guidelines, onboarding documentation.",
              "examples": [
                ["Progressive Disclosure - show basic info first, advanced options on demand", "Immediate Feedback - every action shows visual response within 100ms", "Forgiveness - all destructive actions require confirmation and can be undone"],
                ["Principle of Least Astonishment - extraction results match document structure visually", "Transparency - always show why AI made specific extraction decisions", "Efficiency - keyboard shortcuts for all common actions"],
                ["Discoverability - features accessible within 2 clicks from any starting point", "Consistency - drag-and-drop behavior works identically across all contexts", "Responsiveness - UI remains interactive during long-running operations"]
              ]
            },
            "inspirations": {
              "type": "array",
              "items": {
                "type": "string"
              },
              "description": "Products or design patterns to reference for interaction design, visual language, and user experience. Include specific features or aspects to emulate.",
              "$comment": "Inspirations provide concrete design references, preventing abstract discussions. Format: '{Product} - {specific aspect to emulate}'. Good: 'Notion - smooth inline editing transitions', 'Google Calendar - drag-to-reschedule interaction', 'Linear - keyboard-first command palette'. Bad: 'Notion' (too vague), 'Modern design' (meaningless). Include WHY each inspiration is relevant. Number: 2-5 inspirations. Tools use this for: design system references, competitive analysis, visual design exploration. Designers can pull these products up during design sessions to establish shared vocabulary.",
              "examples": [
                ["Notion - block-based editing with smooth inline transitions", "Google Calendar - drag-and-drop rescheduling with ghost preview", "Slack - real-time collaboration indicators"],
                ["Linear - keyboard-first command palette for power users", "Superhuman - aggressive keyboard shortcuts and speed optimizations", "Figma - multiplayer cursors showing who's working where"],
                ["Airtable - flexible view switching (grid/kanban/gallery) without data loss", "GitHub - inline commenting on specific lines of content", "Stripe Dashboard - real-time event logs for transparency"]
              ]
            },
            "interaction_patterns": {
              "type": "array",
              "items": {
                "type": "string"
              },
              "description": "Specific interaction patterns to implement. Focus on novel or non-obvious patterns that might not emerge naturally.",
              "$comment": "Interaction patterns describe specific UI behaviors, not generic capabilities. Good patterns are specific and implementable. Examples: 'Drag-and-drop with snap-to-grid alignment', 'Command palette (Cmd+K) for all actions', 'Inline editing - double-click to edit any field', 'Hover preview - show full content on hover without navigation', 'Bulk selection via checkbox + shift-click range'. Bad examples: 'Good UX', 'Modern interface', 'Easy to use'. Number: 2-6 patterns. Tools use this for: UI framework selection (does framework support these patterns?), implementation stories (Linear issues for each pattern), accessibility testing (ensure patterns work with keyboard/screen readers). Frontend engineers translate patterns directly into component specs.",
              "examples": [
                ["Drag-and-drop entities between document sections with visual placeholder", "Inline editing - double-click any extracted entity to correct", "Confidence score hover - show extraction reasoning on hover"],
                ["Command palette (Cmd+K) for all actions and navigation", "Bulk operations - checkbox selection with shift-click for ranges", "Keyboard navigation - arrow keys between items, Enter to open, Escape to close"],
                ["Smart auto-save - save draft every 3 seconds during editing", "Undo stack with Cmd+Z for last 20 actions", "Contextual help tooltips - show on first use, then on request"]
              ]
            }
          }
        },
        "contexts": {
          "type": "array",
          "description": "User-facing contexts where this feature appears (UI screens, emails, notifications, API responses, etc.). Contexts help designers and engineers understand WHERE and HOW users interact with capabilities.",
          "$comment": "Contexts are the environments where capabilities become real for users. Each context represents a distinct user touchpoint. UI contexts are screens or views (dashboard, detail page, modal). Email contexts are transactional emails. Notification contexts are alerts/toasts. API contexts are programmatic interfaces. Report contexts are exported/printed documents. Integration contexts are third-party system interfaces. Number: 1-6 contexts typically - most features have 2-4. Tools use this for: UI wireframe generation, email template creation, notification design, API specification (OpenSpec), test scenario generation (each context needs test coverage).",
          "items": {
            "type": "object",
            "required": ["id", "type", "name", "description", "key_interactions", "data_displayed"],
            "properties": {
              "id": {
                "type": "string",
                "pattern": "^ctx-[0-9]+$",
                "description": "Unique context identifier following pattern ctx-###.",
                "$comment": "Context IDs follow format: ctx-001, ctx-002, etc. Unique within feature (not globally). Used for cross-referencing from scenarios (which context does this scenario occur in?). Sequential numbering recommended. Tools use context IDs for: test organization (group tests by context), documentation structure (one section per context), implementation planning (Linear issues per context).",
                "examples": ["ctx-001", "ctx-002", "ctx-003"]
              },
              "type": {
                "type": "string",
                "enum": ["ui", "email", "notification", "api", "report", "integration"],
                "description": "Category of user-facing context. Determines rendering environment and interaction constraints.",
                "$comment": "Context type affects how capabilities are expressed: UI = interactive screens with forms/buttons/navigation, Email = static HTML with limited interactivity and action links, Notification = brief alerts with single CTA, API = programmatic JSON/XML responses with no visual presentation, Report = print/PDF formatted documents, Integration = third-party system interfaces. Most features have UI contexts. Email/notification contexts common for async workflows. API contexts for programmatic access. Tools use type for: template selection (UI vs email have different component libraries), testing strategy (UI=browser, API=request/response, email=rendering check).",
                "examples": ["ui", "email", "notification", "api"]
              },
              "name": {
                "type": "string",
                "minLength": 3,
                "description": "User-recognizable name for this context. Use terms users would understand, not internal system names.",
                "$comment": "Context names should match user mental models. Good UI names: 'Document Upload Screen', 'Extraction Results Dashboard', 'Entity Review Modal'. Bad names: 'DocumentProcessorView', 'ExtractorUI'. For emails: 'Extraction Complete Email', 'Error Notification Email'. For APIs: 'Document Status API', 'Entity Search API'. Length: 2-6 words typically. Tools use names for: documentation headings, test suite organization, user stories ('As a user viewing {context name}...').",
                "examples": ["Document Upload Screen", "Extraction Complete Email", "Entity Review Modal", "Document Status API", "Extraction Progress Notification", "Weekly Compliance Report"]
              },
              "description": {
                "type": "string",
                "minLength": 30,
                "description": "Detailed explanation of this context's purpose, when users encounter it, and what they're trying to accomplish. Should answer: WHEN do users see this? WHAT are they trying to do? WHY is this context needed?",
                "$comment": "Context descriptions establish user intent and context triggers. Good descriptions answer: (1) WHEN users encounter this context (trigger), (2) WHAT they're trying to accomplish (goal), (3) WHY this context exists vs using existing UI (justification). Example: 'Users access this screen after clicking Upload from the main dashboard. They upload one or more compliance documents (PDF/Word) and configure extraction settings (entities to extract, confidence threshold). This dedicated screen exists because upload requires substantial configuration before processing begins.' Length: 2-4 sentences (30-200 chars). Tools use this for: user story generation ('As a user, when I {trigger}, I want to {goal}, so I can {value}'), wireframe annotations, acceptance criteria.",
                "examples": [
                  "Users access this modal after clicking Review on any completed extraction. They verify AI-extracted entities, correct errors, and approve results before saving to database. Modal format enables focused review without losing document context.",
                  "Users receive this email 5-30 minutes after upload when extraction completes successfully. Email shows summary (documents processed, entities extracted, confidence scores) with a CTA link to review results in the app.",
                  "This API endpoint is called by external systems to check extraction status. Returns JSON with processing stage, estimated completion time, and error details if failed."
                ]
              },
              "key_interactions": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "minItems": 1,
                "description": "What users DO in this context. List concrete actions in user language (click, drag, type, etc.), not technical operations.",
                "$comment": "Key interactions are user actions, not system behaviors. Use action verbs: 'Click Upload button', 'Drag PDF file onto drop zone', 'Type document title', 'Select entities to extract from dropdown', 'Review confidence scores in results table'. Bad examples: 'Document processing', 'Database update', 'API call'. Number: 1-5 interactions per context - focus on PRIMARY actions. For email contexts: 'Click View Results link'. For notification contexts: 'Dismiss notification', 'Click Open action'. For API contexts: 'Send POST request with document ID'. Tools use interactions for: UI component selection (button, drag-drop, input field), accessibility testing (keyboard navigation paths), user flow diagrams (arrows between interactions).",
                "examples": [
                  ["Click Upload Documents button", "Drag and drop PDF files onto upload area", "Select entity types to extract from checklist", "Set minimum confidence threshold slider", "Click Start Extraction button"],
                  ["Click View Results link in email body", "Click Dismiss to close notification"],
                  ["Send POST /documents/{id}/extract request", "Poll GET /documents/{id}/status for completion", "Retrieve results via GET /documents/{id}/entities"]
                ]
              },
              "data_displayed": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "minItems": 1,
                "description": "What users SEE in this context. List visible information, not data structures or technical details.",
                "$comment": "Data displayed describes visible information from user perspective. Good examples: 'List of uploaded documents with status badges (Processing/Complete/Error)', 'Confidence score percentage next to each extracted entity', 'Progress bar showing 3 of 5 documents processed'. Bad examples: 'Document objects array', 'Entity database records', 'JSON response'. Number: 2-6 data elements - focus on CRITICAL information users need to accomplish their goal. For email: 'Document title', 'Extraction summary (X entities from Y pages)', 'CTA button text'. For API: 'Status code', 'Entity array with confidence scores', 'Error message if failed'. Tools use this for: UI component selection (table, list, card, badge), data fetching requirements (what API queries needed?), test assertions (verify correct data shown).",
                "examples": [
                  ["List of uploaded documents with status indicators", "Extraction progress percentage per document", "Estimated time remaining", "Error messages if upload failed"],
                  ["Document title and upload date", "Summary: X entities extracted from Y pages", "Confidence score distribution chart", "View Results button"],
                  ["HTTP status code (200/202/400/500)", "JSON array of extracted entities with confidence scores", "Processing status (pending/processing/complete/error)", "Error details object if failed"]
                ]
              }
            }
          }
        },
        "scenarios": {
          "type": "array",
          "minItems": 1,
          "description": "Concrete user scenarios showing how capabilities are used in context. Each scenario follows actor → context → trigger → action → outcome pattern with testable acceptance criteria.",
          "$comment": "Scenarios are end-to-end user flows that demonstrate feature value. They connect abstract capabilities to concrete experiences. Format: (1) Actor - who is doing this, (2) Context - where/when it happens, (3) Trigger - what starts the flow, (4) Action - step-by-step what happens, (5) Outcome - value delivered, (6) Acceptance criteria - testable success conditions. Number: 2-6 scenarios typically - cover happy path, error cases, edge cases. Tools use this for: BDD test generation (Given/When/Then from scenario structure), user story creation ('As {actor}, when {trigger}, I want to {action}, so I can {outcome}'), demo script generation, QA test case creation.",
          "items": {
            "type": "object",
            "required": ["id", "name", "actor", "context", "trigger", "action", "outcome", "acceptance_criteria"],
            "properties": {
              "id": {
                "type": "string",
                "pattern": "^scn-[0-9]+$",
                "description": "Unique scenario identifier following pattern scn-###.",
                "$comment": "Scenario IDs follow format: scn-001, scn-002, etc. Unique within feature. Sequential numbering recommended. Often ordered by importance (happy path first, edge cases last). Tools use scenario IDs for: test organization (one test suite per scenario), documentation cross-references, traceability matrices (which capabilities does this scenario exercise?).",
                "examples": ["scn-001", "scn-002", "scn-003"]
              },
              "name": {
                "type": "string",
                "minLength": 10,
                "description": "Human-readable scenario name summarizing the user goal. Use format: '{Actor} {Action} {Object}' or '{Goal Achievement}'.",
                "$comment": "Scenario names should immediately convey what's happening and who benefits. Good formats: 'Compliance Officer uploads quarterly audit document', 'Extract entities from multi-page PDF with tables', 'Recover from extraction timeout error'. Bad names: 'Document upload', 'Happy path', 'Test scenario 1'. Length: 3-8 words. Tools use names for: test suite names, documentation headings, demo script titles.",
                "examples": [
                  "Compliance Officer uploads quarterly audit document",
                  "Extract entities from multi-page PDF with embedded tables",
                  "Review and correct AI extraction errors before saving",
                  "Receive email notification when extraction completes",
                  "Recover from document processing timeout error"
                ]
              },
              "jtbd_category": {
                "type": "string",
                "description": "Optional reference to the job-to-be-done category this scenario implements. Enables grouping scenarios by user intent.",
                "$comment": "JTBD category links scenarios back to feature definition. Multiple scenarios can serve same JTBD (happy path, error recovery, edge cases). Example: JTBD 'When I receive compliance documents, I want to extract key entities, so I can assess impact quickly' might have scenarios: 'Extract from clean PDF', 'Extract from scanned image', 'Handle extraction errors'. Tools use this for: scenario organization, coverage analysis (do all JTBDs have scenarios?), prioritization (JTBD with no scenarios might be underserved).",
                "examples": [
                  "Extract compliance entities",
                  "Review extraction results",
                  "Integrate with document management",
                  "Monitor extraction progress"
                ]
              },
              "actor": {
                "type": "string",
                "description": "Who performs this scenario. Use specific role titles matching personas when possible.",
                "$comment": "Actor should match one of the 4 personas from the personas section for consistency. Using same actor names creates cohesion across feature definition. If scenario involves system-to-system interaction, use 'System' or 'Scheduled Job'. Examples: 'Compliance Officer' (matches persona), 'Board Secretary', 'Senior Engineer', 'System' (for automated scenarios). Tools use actor for: test role assignments, demo script character assignments, documentation persona filters.",
                "examples": [
                  "Compliance Officer",
                  "Board Secretary",
                  "Senior Engineer",
                  "System",
                  "Scheduled Job"
                ]
              },
              "context": {
                "type": "string",
                "minLength": 20,
                "description": "Where and when this scenario occurs. Describe the situation, preconditions, and any relevant background. Should answer: WHAT has happened before this? WHERE is the user? WHAT state is the system in?",
                "$comment": "Context establishes scenario starting conditions. Good context: 'Officer is on main dashboard with 10 documents from last quarter. Quarterly audit is due in 2 days. They need to extract obligations from new FDA guidance document just received.' Bad context: 'User is logged in'. Length: 1-3 sentences (20-150 chars). Tools use this for: test setup (preconditions to establish before running scenario), demo script setup (what to show before the scenario), Given clauses in BDD tests.",
                "examples": [
                  "Officer is on main dashboard after morning email check. They have 8 documents to process before quarterly board meeting in 3 days. A critical compliance update document just arrived requiring immediate analysis.",
                  "Engineer is reviewing extraction results from overnight batch processing. Out of 50 documents, 3 show low confidence scores (<60%) requiring manual review. They need to correct errors before legal team review at 2pm.",
                  "System has completed document processing. Extraction took 8 minutes for 25-page PDF with complex tables. All entities extracted with >85% confidence. User notification is queued."
                ]
              },
              "trigger": {
                "type": "string",
                "minLength": 10,
                "description": "What initiates this scenario. Can be user action, system event, scheduled task, or external signal.",
                "$comment": "Trigger is the entry point to the scenario. Good triggers are specific: 'User clicks Upload Documents button', 'Scheduled job runs at 2am daily', 'Extraction API completes processing', 'Error threshold exceeded (3 failures in 5 minutes)'. Bad triggers: 'User does something', 'System starts'. Tools use triggers for: event mapping (which events start which scenarios?), API endpoint identification (which endpoint is the trigger?), monitoring/alerting (set up alerts on these trigger conditions).",
                "examples": [
                  "User clicks Upload Documents button in main dashboard toolbar",
                  "Extraction service completes processing and publishes 'ExtractionComplete' event",
                  "Scheduled job runs at 2am daily to process overnight document queue",
                  "Confidence score drops below 60% threshold triggering manual review alert",
                  "User receives email notification and clicks View Results link"
                ]
              },
              "action": {
                "type": "string",
                "minLength": 20,
                "description": "Step-by-step description of actions taken by actor and system responses. Use numbered steps or narrative flow. Focus on observable behaviors, not internal implementation.",
                "$comment": "Action is the main scenario body. Can be structured (1. User does X, 2. System responds Y) or narrative ('User drags PDF onto upload area. Drop zone highlights. User configures settings. System validates file and starts extraction...'). Length: 3-8 steps or 50-200 chars narrative. Tools use action for: test step generation (each action = test step), UI wireframe flow (arrows between screens), API sequence diagrams (each action = API call), documentation illustrations.",
                "examples": [
                  "1. User drags PDF file onto upload drop zone. 2. Drop zone highlights and shows file preview. 3. User selects entity types to extract (Person, Organization, Date, Obligation). 4. User sets minimum confidence threshold to 70%. 5. User clicks Start Extraction. 6. System validates file (PDF, <50MB, <100 pages) and queues for processing. 7. Progress modal appears showing real-time status.",
                  "System receives ExtractionComplete event with document ID and entity results. System calculates confidence score distribution. If any entity <60% confidence, system flags for manual review. System generates email notification with summary (X entities from Y pages, confidence range A-B%). System sends email and records notification in audit log.",
                  "User opens extraction results modal. Results table shows 47 entities with confidence scores. User notices 'Acme Corp' incorrectly extracted as 'Acne Corp'. User double-clicks entity name to edit. Inline editor appears. User corrects to 'Acme Corp'. System re-validates entity and updates confidence to 95%. User clicks Save All. System persists corrections and timestamps changes."
                ]
              },
              "outcome": {
                "type": "string",
                "minLength": 20,
                "description": "Expected result after scenario completes successfully. Describe both system state changes and user value delivered. Should answer: WHAT changed? WHAT value did user get? WHAT can they do now?",
                "$comment": "Outcome captures both observable results and delivered value. Good outcome: 'Document uploaded successfully. Extraction processing started. User sees progress modal updating every 3 seconds. Expected completion in 5-10 minutes. User can continue working while extraction runs in background. Email notification will arrive when complete.' Bad outcome: 'Success'. Length: 2-4 sentences (20-150 chars). Tools use outcome for: test assertions (verify these results), value tracking (measure these outcomes in production), demo script endings (what to highlight at scenario end).",
                "examples": [
                  "Document processing completes successfully. 47 entities extracted with average confidence 87%. User reviewed and corrected 3 low-confidence entities (<70%). All entities now saved to database with audit trail. Document status changed to 'Reviewed'. User can now proceed with compliance analysis using clean, validated entity data.",
                  "User receives email 8 minutes after upload. Email shows: 'Compliance Document Q4-2023 processed successfully. 47 entities extracted (12 Persons, 15 Organizations, 8 Dates, 12 Obligations). Confidence scores: 87% average, 3 flagged for review.' User clicks View Results link and lands on entity review page with flagged items highlighted.",
                  "Extraction job times out after 30 minutes (PDF has 200 pages exceeding limit). System marks job as failed. User receives email notification explaining timeout. Email includes: error details, partial results if any, suggestions (split document into smaller files), retry button. User understands issue and knows how to proceed."
                ]
              },
              "acceptance_criteria": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "minItems": 1,
                "description": "Testable conditions that MUST be true for scenario to be considered successful. Write in Given/When/Then or checklist format.",
                "$comment": "Acceptance criteria are objective success conditions. Good criteria are: (1) Specific - no ambiguity, (2) Measurable - can verify true/false, (3) Testable - can be automated in test suite. Format options: 'Given {precondition} When {action} Then {result}' or checklist '- Document upload completes in <30 seconds', '- All entities have confidence scores >0%'. Number: 3-8 criteria per scenario. Tools use criteria for: automated test assertions (each criterion = one or more assertions), QA checklists, acceptance testing, documentation examples.",
                "examples": [
                  [
                    "Given user has uploaded a valid PDF (<50MB, <100 pages) When extraction completes Then all configured entity types are present in results",
                    "Given extraction is processing When user views progress modal Then progress updates at least every 5 seconds",
                    "Given extraction completes When entities are displayed Then each entity shows confidence score as percentage 0-100%",
                    "Given user corrects an entity When they save changes Then confidence score updates to reflect correction (typically >90%)",
                    "Given extraction is processing When user navigates away Then extraction continues in background and email is sent on completion"
                  ],
                  [
                    "- Extraction completes within 2x estimated time (e.g., 10 min document should finish in <20 min)",
                    "- Email notification sent within 1 minute of extraction completion",
                    "- All entities in results have non-null confidence scores (0-100%)",
                    "- Low-confidence entities (<70%) are flagged with warning indicator",
                    "- View Results link in email navigates to correct document review page"
                  ]
                ]
              }
            }
          }
        },
        "external_integrations": {
          "type": "array",
          "description": "Third-party systems this feature connects to. Include only integrations essential to feature operation, not general platform capabilities.",
          "$comment": "External integrations document dependencies on systems outside our control. Include: (1) SaaS products we call (Google Calendar, Slack, Stripe), (2) Internal services this feature depends on (Auth service, Notification service), (3) Systems that call us (webhooks from external platforms). Don't include: browser APIs, standard libraries, infrastructure (databases, queues). Number: 0-4 integrations typically. Simple features often have 0 external integrations. Tools use this for: architecture diagrams (show external system boundaries), dependency analysis (what breaks if integration fails?), security review (data leaving system), rate limit planning.",
          "items": {
            "type": "object",
            "required": ["name", "purpose"],
            "properties": {
              "name": {
                "type": "string",
                "description": "Name of the external system. Use official product name, not internal nickname.",
                "$comment": "System names should be recognizable to stakeholders. Good: 'Google Calendar API', 'Slack Webhooks', 'Stripe Payment API', 'Auth0', 'Twilio SMS'. Bad: 'The calendar thing', 'Chat system', 'Payments'. For internal services: use service name from architecture diagrams. Tools use name for: integration health monitoring (track this system's availability), documentation generation (external system reference section), security scanning (data flows to these systems).",
                "examples": [
                  "Google Calendar API",
                  "Slack Webhooks",
                  "Stripe Payment API",
                  "Auth0 Authentication",
                  "Twilio SMS",
                  "Internal Notification Service"
                ]
              },
              "purpose": {
                "type": "string",
                "minLength": 20,
                "description": "Why this integration exists and what value it provides. Explain what would break or be missing without this integration.",
                "$comment": "Purpose explains integration necessity. Good purpose: 'Sends real-time extraction completion notifications to team Slack channels. Without this, users would need to manually check for completed extractions or wait for email notifications (slower).' Bad purpose: 'Sends notifications'. Length: 1-3 sentences (20-150 chars). Tools use purpose for: architecture decision records (why we chose this integration vs alternatives), impact analysis (if integration fails, what functionality is lost?), cost/benefit evaluation.",
                "examples": [
                  "Sends real-time extraction completion notifications to team Slack channels. Enables immediate team coordination when compliance documents are processed. Without this, users rely solely on email notifications which are checked less frequently.",
                  "Authenticates users via OAuth 2.0. Provides single sign-on across all company applications. Manages user sessions, permissions, and MFA. Without this, we'd need to build custom authentication (significant security risk).",
                  "Extracts text and tables from PDF documents using OCR. Handles scanned images and complex layouts that basic PDF parsers cannot process. Without this, feature would only work on text-based PDFs (excludes ~30% of user documents)."
                ]
              },
              "direction": {
                "type": "string",
                "enum": ["inbound", "outbound", "bidirectional"],
                "description": "Data flow direction: inbound (they call us), outbound (we call them), or bidirectional (both).",
                "$comment": "Direction affects: (1) Security posture - inbound means they need our API credentials, outbound means we store their credentials, (2) Reliability - outbound failures retry on our side, inbound failures depend on their retry logic, (3) Rate limiting - outbound we control request rate, inbound they control rate. Inbound examples: webhooks from Stripe, callbacks from OAuth providers. Outbound examples: we call Slack API, we call Twilio SMS. Bidirectional: we both call each other (complex integration). Tools use direction for: rate limit planning, authentication setup (API keys vs OAuth), monitoring (different metrics for inbound vs outbound).",
                "examples": ["inbound", "outbound", "bidirectional"]
              }
            }
          }
        }
      }
    },
    "boundaries": {
      "type": "object",
      "description": "Explicit boundaries defining what this feature does NOT do (non_goals) and what limitations it operates under (constraints). Critical for scope management and stakeholder alignment.",
      "$comment": "Boundaries prevent scope creep and misaligned expectations. Non-goals clarify what's intentionally excluded. Constraints document unavoidable limitations. Both are consumed by: Product managers (scope conversations), Engineers (design decisions), Sales (capability discussions), Documentation (limitations sections). Boundaries are as important as capabilities - they prevent wasted effort on out-of-scope work.",
      "properties": {
        "non_goals": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "minItems": 1,
          "description": "What this feature explicitly does NOT do. List capabilities that might be expected but are intentionally excluded. Explain WHY each is a non-goal.",
          "$comment": "Non-goals prevent scope creep by explicitly stating exclusions. Good non-goals: (1) Sound reasonable at first (otherwise why mention?), (2) Include rationale (why excluded?), (3) Point to alternatives if available. Format: '{Excluded capability} - {reason for exclusion} - {alternative if any}'. Examples: 'Real-time collaborative editing - current user base too small to justify WebSocket infrastructure - users can save and refresh', 'Mobile app - 90% of usage is desktop - responsive web app sufficient for now'. Number: 2-6 non-goals typically. Tools use this for: requirement rejection justification (point to non-goal during scope discussions), roadmap planning (non-goals become future features), sales enablement (don't promise excluded capabilities).",
          "examples": [
            [
              "Real-time collaborative editing - Current user base (<100 active users) doesn't justify WebSocket infrastructure complexity. Users can save drafts and refresh to see others' changes. May revisit when userbase exceeds 500.",
              "Mobile native app - 90% of current usage is desktop during business hours. Responsive web app provides sufficient mobile experience for occasional use. Native app not planned until mobile usage exceeds 20%.",
              "Automated extraction without human review - Regulatory requirements mandate human verification of all AI-extracted compliance data. Full automation would violate audit requirements. Review step is non-negotiable."
            ],
            [
              "OCR for handwritten documents - Handwriting recognition accuracy <60% for compliance documents (tested with sample set). Excluded to prevent unreliable results. Users must submit typed/printed documents only.",
              "Bulk document upload (>10 documents) - Current extraction service scales to 10 concurrent documents max. Bulk upload would require infrastructure upgrade ($50k+). Single/small batch upload sufficient for MVP.",
              "Version control / document history - Adds significant complexity (30+ engineering days) for use case affecting <5% of users. Users can manually track versions in filenames or external systems. May add in v2.0 if heavily requested."
            ]
          ]
        },
        "constraints": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "Technical, business, regulatory, or resource limitations that constrain feature design and implementation. These are unavoidable restrictions that must be worked within.",
          "$comment": "Constraints document inescapable limitations affecting feature capabilities. Types: (1) Technical - system limitations (max file size, processing time, API rate limits), (2) Regulatory - compliance requirements (GDPR, HIPAA, SOX), (3) Business - budget/timeline/staffing restrictions, (4) Third-party - external service limitations. Format: '{Constraint type}: {specific limitation} - {impact on feature}'. Examples: 'Technical: PDF size limited to 50MB due to Lambda 10min timeout - documents over 50MB must be split', 'Regulatory: All extraction results must be stored in EU region for GDPR compliance - affects database architecture'. Number: 2-6 constraints typically. Tools use this for: architecture decisions (design around constraints), error messages (explain constraint violations to users), capacity planning (monitor constraint thresholds), documentation (limitations section).",
          "examples": [
            [
              "Technical: PDF size limited to 50MB due to AWS Lambda 10-minute timeout constraint. Documents exceeding 50MB must be split into smaller files before upload. Affects ~2% of user documents based on historical data.",
              "Regulatory: All extracted compliance data must be stored in EU region for GDPR compliance. US-based extraction services cannot be used. Limits choice of AI/ML providers to EU-hosted options only.",
              "Business: Total extraction budget is $5000/month ($0.10 per document, 50k documents max). Beyond this threshold, manual processing queue is used. Affects feature scalability - cannot support sudden spikes without budget increase.",
              "Third-party: Google Cloud Vision API rate limit is 1800 requests/minute. Extraction must throttle requests to stay under limit. During high-load periods (month-end), processing may slow to 1800 documents/minute max."
            ],
            [
              "Technical: Extraction processing time scales linearly with page count (~30 seconds per page). Documents over 100 pages will take 50+ minutes. UI must handle long-running operations gracefully.",
              "Regulatory: User correction data must be logged for audit trail (SOX requirement). Adds ~15% storage overhead. Cannot optimize by discarding correction history.",
              "Business: MVP timeline is 3 months, limiting implementation to essential features only. Advanced analytics and reporting deferred to v2.0. May require additional manual workarounds in interim.",
              "Resource: Team has 1 ML engineer part-time (20 hours/week). Complex model training or custom entity types require external consulting ($150/hour). Limits ability to rapidly iterate on extraction accuracy improvements."
            ]
          ]
        }
      }
    },
    "dependencies": {
      "type": "object",
      "description": "Dependencies on other features within the product ecosystem. Documents both blocking dependencies (requires) and enabling relationships (enables). Essential for sequencing and impact analysis.",
      "$comment": "Dependencies create feature DAG (directed acyclic graph) for implementation planning. 'requires' = features that must exist BEFORE this one can work. 'enables' = features that become possible AFTER this one exists. Format: rich objects with id (cross-reference), name (human-readable), reason (explains relationship). Tools use this for: implementation ordering (topological sort), impact analysis (what breaks if dependency removed?), roadmap planning (show dependency chains), documentation (prerequisite sections).",
      "properties": {
        "requires": {
          "type": "array",
          "description": "Features that must exist before this feature can be implemented. These are blocking dependencies - this feature cannot function without them.",
          "$comment": "Required dependencies answer: 'What must work before this can work?' Good requires: (1) Specific - reference exact feature by ID, (2) Justified - explain what capability is needed, (3) Unavoidable - dependency cannot be worked around. Example: Feature 'Advanced Search' requires 'Basic Search' (fd-002) because 'Advanced filters (date range, entity type, confidence score) extend basic search UI and query engine - cannot implement without base search infrastructure'. Bad example: 'Needs authentication' (too vague). Number: 0-4 dependencies typically. Most features have 0-2 hard dependencies. Tools use requires for: build order (implement dependencies first), blocker tracking (show blocked features on roadmap), risk assessment (dependency on unbuilt feature = risk).",
          "items": {
            "type": "object",
            "required": ["id", "name", "reason"],
            "properties": {
              "id": {
                "type": "string",
                "pattern": "^fd-[0-9]+$",
                "description": "Feature definition ID of the required feature. Must reference existing feature definition.",
                "$comment": "ID creates hard link between features. Format: fd-### (e.g., fd-001, fd-002). Must match an actual feature definition file (fd-001-some-feature.yaml). Tools validate this reference exists. Used for: dependency graph visualization (draw arrows between features), circular dependency detection (requires cannot form cycles), cross-reference navigation (jump from this feature to dependency definition).",
                "examples": ["fd-001", "fd-002", "fd-015", "fd-023"]
              },
              "name": {
                "type": "string",
                "minLength": 10,
                "description": "Human-readable name of the required feature. Should match the name field in the referenced feature definition.",
                "$comment": "Name provides human context when reading dependency without looking up full feature. Should match exactly the name field in fd-{id}.yaml for consistency. If dependency feature is renamed, update this name too. Tools use name for: dependency lists in documentation ('This feature requires: Document Upload, Entity Extraction'), tooltips in UI (hover over dependency to see name), natural language explanations.",
                "examples": [
                  "Document Upload and Storage",
                  "Basic Entity Extraction",
                  "User Authentication and Authorization",
                  "Real-time Notification System"
                ]
              },
              "reason": {
                "type": "string",
                "minLength": 30,
                "description": "Detailed explanation of WHY this dependency exists. Must explain what specific capability from the required feature is needed and how it's used.",
                "$comment": "Reason justifies the dependency and helps engineers understand coupling. Good reason: 'Requires Document Upload (fd-001) because extraction service needs access to uploaded PDF files in cloud storage. Extraction cannot start until document is uploaded, validated, and storage URL is available.' Bad reason: 'Needs documents'. Length: 1-3 sentences (30-200 chars). Tools use reason for: architecture documentation (explain coupling), alternative exploration (could dependency be removed with different design?), impact analysis (if required feature changes, what breaks?).",
                "examples": [
                  "Requires Document Upload (fd-001) because extraction service processes uploaded PDF files. Cannot extract entities without documents in storage. Uses document ID and storage URL from upload feature.",
                  "Requires User Authentication (fd-005) to determine which extracted entities user has permission to view. Entity visibility is scoped by organization membership established during authentication. Without auth, cannot enforce multi-tenant data isolation.",
                  "Requires Real-time Notifications (fd-012) to alert users when extraction completes. Extraction runs asynchronously (5-30 minutes). Without notifications, users would need to manually poll for completion, creating poor UX."
                ]
              }
            }
          }
        },
        "enables": {
          "type": "array",
          "description": "Features that become possible once this feature exists. These are forward dependencies - building this feature unlocks future capabilities.",
          "$comment": "Enables relationships answer: 'What becomes possible after this exists?' Good enables: (1) Specific - reference exact feature by ID, (2) Justified - explain what NEW capability this provides to enabled feature, (3) Meaningful - enabled feature adds significant value on top of this one. Example: Feature 'Basic Search' enables 'Advanced Search' (fd-015) because 'Basic search provides query engine and UI framework that advanced filters extend. Building block for complex search scenarios'. Number: 0-6 enabled features typically. Foundation features (auth, storage) enable many others. Specialized features may enable 0. Tools use enables for: roadmap sequencing (implement this to unblock multiple features), value prioritization (features that enable many others = high leverage), documentation (show what's possible after this ships).",
          "items": {
            "type": "object",
            "required": ["id", "name", "reason"],
            "properties": {
              "id": {
                "type": "string",
                "pattern": "^fd-[0-9]+$",
                "description": "Feature definition ID of the enabled feature. Must reference existing or planned feature definition.",
                "$comment": "ID creates forward link to dependent feature. Format same as requires. May reference features not yet built (planned features). Tools use for: forward dependency tracking (what future features depend on this?), leverage analysis (features enabling many others = high value), prioritization (build high-leverage features first). Circular dependency detection also checks enables relationships.",
                "examples": ["fd-015", "fd-020", "fd-025", "fd-030"]
              },
              "name": {
                "type": "string",
                "minLength": 10,
                "description": "Human-readable name of the enabled feature. Should match the name field in the referenced feature definition.",
                "$comment": "Name provides context. Same rules as requires.name. Tools use for: forward dependency lists in documentation ('This feature enables: Advanced Search, Bulk Processing, Compliance Reporting'), value storytelling (show cascade of value this feature unlocks), roadmap narrative.",
                "examples": [
                  "Advanced Entity Search",
                  "Bulk Document Processing",
                  "Compliance Analytics Dashboard",
                  "Automated Workflow Triggers"
                ]
              },
              "reason": {
                "type": "string",
                "minLength": 30,
                "description": "Detailed explanation of HOW this feature enables the other. Must explain what specific capability this provides that the enabled feature builds upon.",
                "$comment": "Reason explains the enabling relationship from this feature's perspective. Good reason: 'Enables Advanced Search (fd-015) by providing query engine and indexed entity data. Advanced search adds filters (date range, confidence score, entity type) on top of this foundation. Without basic search infrastructure, advanced search would need to rebuild query engine from scratch.' Bad reason: 'Makes search possible'. Length: 1-3 sentences (30-200 chars). Tools use reason for: architectural storytelling (show how features build on each other), refactoring analysis (if this feature changes, what enabled features need updates?), value narrative (explain cascade effects).",
                "examples": [
                  "Enables Advanced Search (fd-015) by providing indexed entity database and query API. Advanced search adds complex filters (confidence scores, entity types, date ranges) on top of this search foundation. Without basic search, advanced features would need to rebuild entire query infrastructure.",
                  "Enables Bulk Processing (fd-020) by establishing document upload pipeline and extraction service architecture. Bulk processing extends this with queue management and parallel processing. Core extraction logic is reused - bulk feature adds orchestration layer only.",
                  "Enables Compliance Dashboard (fd-025) by extracting and structuring compliance entities (obligations, deadlines, requirements). Dashboard visualizes this structured data. Without entity extraction, dashboard would need to process raw documents (computationally expensive and error-prone)."
                ]
              }
            }
          }
        }
      }
    },
    "implementation_references": {
      "type": "object",
      "description": "Bi-directional traceability between EPF feature definition and implementation specs created by external spec-driven tools (OpenSpec, Cursor Composer, etc.). Tracks which capabilities/scenarios have corresponding implementation specs.",
      "$comment": "implementation_references closes the loop between product vision (EPF) and implementation reality (spec-driven tools). EPF defines WHAT to build (capabilities, scenarios). Spec tools define HOW to build it (API endpoints, database schemas, test cases). This section links the two worlds. Tools use this for: coverage tracking (what % of feature is implemented?), traceability (jump from capability to implementation), synchronization (detect when EPF changes affect implementation), status rollup (are all scenarios tested?). Maintained by spec-driven tools via bi-directional sync - see integration_specification.yaml Section 12 for update protocol.",
      "properties": {
        "tool_name": {
          "type": "string",
          "description": "Name of the spec-driven tool that created/maintains the linked implementation specs. Must match registered tool name from integration_specification.yaml.",
          "$comment": "tool_name identifies which tool ecosystem owns the implementation specs. Common values: 'openspec' (API-first spec system), 'cursor-composer' (Cursor IDE agent specs), 'specit' (behavior-driven specs), 'linear-specs' (Linear issue-based specs). Used for: routing sync updates (tool knows to update its own specs), icon/branding in UI (show tool logo next to spec link), tool-specific actions (each tool may have custom sync/validation commands). If multiple tools implement same feature, create separate implementation_references entries.",
          "examples": ["openspec", "cursor-composer", "specit", "linear-specs", "aider-architect"]
        },
        "specs": {
          "type": "array",
          "description": "List of individual implementation spec files or entries created by the tool. Each spec covers one or more EPF capabilities/scenarios.",
          "$comment": "specs array contains granular spec-to-capability mappings. One feature may have multiple implementation specs (e.g., frontend spec, backend spec, infrastructure spec). Each spec entry tracks: where to find it (path/url), what it covers (capability_coverage, scenario_coverage), implementation status (planned → in-progress → implemented → tested). Tools use this for: direct navigation (click cap-001 → jump to implementing spec), coverage visualization (show which capabilities have specs), work tracking (how many specs in-progress?), regression testing (which specs to re-run when EPF changes).",
          "items": {
            "type": "object",
            "required": ["id"],
            "properties": {
              "id": {
                "type": "string",
                "description": "Unique identifier for this spec within the tool's system. Format is tool-specific (OpenSpec uses SPEC-001, Linear uses LIN-1234, etc.).",
                "$comment": "id is the primary key for bi-directional lookup. EPF uses id to reference spec. Tool uses id to find which EPF feature owns the spec. Format varies by tool: OpenSpec: SPEC-001, Cursor: cursor://composer/spec-abc123, Linear: project-LIN-1234, GitHub: org/repo#issue-42. Must be stable (don't change after creation) and unique within tool. Tools use id for: sync updates (match incoming spec update to EPF feature), linking (construct URL from id), deduplication (detect duplicate spec registrations).",
                "examples": [
                  "SPEC-001",
                  "cursor://composer/spec-abc123",
                  "linear-LIN-1234",
                  "github-acme/product#42",
                  "specit-feat-document-upload-backend"
                ]
              },
              "path": {
                "type": "string",
                "description": "Relative path to spec file from repository root. Use for local/repository-hosted specs. Mutually exclusive with url (use one or the other).",
                "$comment": "path enables direct file access for repo-local specs. Good practice: store specs near implementation code (openspec/features/upload.md or src/features/upload/spec.yaml). Path must be relative to repo root (no leading /). Use forward slashes even on Windows. Tools use path for: file opening (VSCode: open spec file in editor), git operations (track spec changes alongside code), validation (check if spec file exists), IDE integration (show spec preview in sidebar).",
                "examples": [
                  "openspec/features/document-upload.md",
                  "specs/extraction/entity-extraction.yaml",
                  "docs/implementation/upload-api.md",
                  ".cursor/specs/frontend-upload-ui.md"
                ]
              },
              "url": {
                "type": "string",
                "format": "uri",
                "description": "Full URL if spec is hosted externally (GitHub issue, Notion page, Linear ticket, etc.). Mutually exclusive with path.",
                "$comment": "url supports external spec systems (Linear, Notion, GitHub Issues, Confluence, etc.). Use when specs live outside repository. Must be full URL (include protocol: https://). If tool has web UI, prefer direct spec URL (Linear: linear.app/acme/issue/LIN-1234) over API URL. Tools use url for: deep linking (click → open in browser/tool), validation (check URL is reachable), integration (fetch spec content via API if available), external sync (webhook from external tool back to EPF).",
                "examples": [
                  "https://linear.app/acme/issue/LIN-1234/document-upload-api",
                  "https://github.com/acme/product/issues/42",
                  "https://notion.so/acme/Document-Upload-Spec-abc123",
                  "https://acme.atlassian.net/browse/PROJ-789"
                ]
              },
              "capability_coverage": {
                "type": "array",
                "items": {
                  "type": "string",
                  "pattern": "^cap-[0-9]+$"
                },
                "description": "List of EPF capability IDs (cap-001, cap-002, etc.) that this spec implements. Enables capability-to-spec traceability.",
                "$comment": "capability_coverage maps spec to EPF capabilities, answering 'Which capabilities does this spec implement?' One spec may cover multiple capabilities (e.g., backend spec implements cap-001 validation + cap-002 storage). Tools use this for: coverage reporting (5 of 8 capabilities have specs), impact analysis (capability changed → which specs need updates?), work planning (prioritize specs for high-value capabilities), documentation generation (embed spec link in capability section). Update this when: (1) new capability added to spec, (2) capability removed from spec, (3) EPF capability IDs renumbered.",
                "examples": [
                  ["cap-001", "cap-002", "cap-003"],
                  ["cap-005"],
                  ["cap-010", "cap-011", "cap-012", "cap-013"]
                ]
              },
              "scenario_coverage": {
                "type": "array",
                "items": {
                  "type": "string",
                  "pattern": "^scn-[0-9]+$"
                },
                "description": "List of EPF scenario IDs (scn-001, scn-002, etc.) that have test coverage in this spec. Enables scenario-to-test traceability.",
                "$comment": "scenario_coverage maps spec to EPF scenarios, answering 'Which scenarios have tests in this spec?' Critical for: test coverage tracking (7 of 10 scenarios have E2E tests), regression testing (scenario changed → re-run these tests), acceptance criteria verification (all scenarios must be tested before shipping). One spec may have tests for multiple scenarios. E2E specs typically cover many scenarios. Unit test specs may cover partial scenarios (one test per scenario step). Tools use this for: test reports (map failing tests back to scenarios), coverage visualization (red/yellow/green scenario badges), acceptance signoff (PM approves when all scenarios green).",
                "examples": [
                  ["scn-001", "scn-002"],
                  ["scn-003", "scn-004", "scn-005"],
                  ["scn-001"]
                ]
              },
              "status": {
                "type": "string",
                "enum": ["planned", "in-progress", "implemented", "tested"],
                "description": "Current implementation status of this spec. Updated by spec-driven tool as work progresses.",
                "$comment": "status tracks spec lifecycle: planned = spec created but no code yet, in-progress = actively implementing, implemented = code written but no tests, tested = tests passing. Tools update status automatically (e.g., when tests pass, set to 'tested'). Status aggregates up to coverage_summary. Status transitions: planned → in-progress (start coding) → implemented (code complete) → tested (tests pass). Status may regress (tested → in-progress if tests break). Tools use status for: kanban boards (columns by status), burndown charts (track progress), blockers (can't ship until all specs 'tested'), notifications (alert when spec status changes).",
                "examples": ["planned", "in-progress", "implemented", "tested"]
              }
            }
          }
        },
        "last_sync": {
          "type": "string",
          "format": "date-time",
          "description": "ISO 8601 timestamp of last sync from spec-driven tool. Used to detect stale data and trigger re-sync.",
          "$comment": "last_sync tracks freshness of implementation_references. Spec-driven tools update this field every time they sync status back to EPF. If last_sync is >7 days old, data may be stale (tool stopped syncing, sync webhook broken, manual sync needed). Tools use this for: staleness warnings (show red flag if >7 days), sync scheduling (auto-sync every 24 hours if older), troubleshooting (when did sync stop working?). Format: ISO 8601 with timezone (2024-01-15T14:30:00Z). Updated by tool's sync process, not by humans.",
          "examples": [
            "2024-01-15T14:30:00Z",
            "2024-12-23T09:15:30Z",
            "2025-03-10T18:45:00Z"
          ]
        },
        "coverage_summary": {
          "type": "object",
          "description": "Aggregated coverage statistics computed from specs array. Provides quick overview of implementation completeness without scanning all specs.",
          "$comment": "coverage_summary is derived data (computed from specs array) but cached here for performance. Answers key questions: (1) What % of capabilities are implemented? (2) What % of scenarios are tested? (3) Is feature ready to ship? Tools recompute summary whenever specs array changes. Summary drives: progress bars in UI, shipment decisions (must have 100% scenario_coverage), prioritization (low coverage = needs attention), reports (feature X is 75% complete).",
          "properties": {
            "capabilities_covered": {
              "type": "integer",
              "description": "Number of unique EPF capabilities that have at least one spec (count of unique cap IDs in all capability_coverage arrays). Max value = capabilities_total.",
              "$comment": "capabilities_covered counts how many capabilities have ANY implementation. Capability with 3 specs counts as 1 (not 3). Used for: coverage % = covered/total, warning if covered < total (some capabilities have no specs yet). Tools compute by: flatten all capability_coverage arrays, count unique cap IDs. Example: specs cover [cap-001, cap-002] and [cap-002, cap-003] → covered = 3 (unique IDs).",
              "examples": [5, 8, 12, 20]
            },
            "capabilities_total": {
              "type": "integer",
              "description": "Total number of EPF capabilities defined in this feature (count of items in implementation.capabilities array). This is the denominator for coverage %.",
              "$comment": "capabilities_total is ground truth from EPF feature definition. Tools read this from count(implementation.capabilities). Used for: coverage % calculation (covered/total * 100), completeness checks (covered must equal total before shipping), reports (Feature X has 8/12 capabilities implemented = 67%). Should never change unless EPF feature definition changes (new capability added or removed).",
              "examples": [8, 12, 15, 25]
            },
            "scenarios_tested": {
              "type": "integer",
              "description": "Number of unique EPF scenarios that have test coverage (count of unique scn IDs in all scenario_coverage arrays). Max value = scenarios_total.",
              "$comment": "scenarios_tested counts how many scenarios have ANY tests. Scenario with 5 test specs counts as 1 (not 5). Critical metric for: acceptance criteria (all scenarios must be tested), regression confidence (tested scenarios unlikely to break), QA signoff (QA approves when scenarios_tested = scenarios_total). Tools compute by: flatten all scenario_coverage arrays, count unique scn IDs. More important than capabilities_covered because scenarios = user-facing behavior.",
              "examples": [3, 7, 10, 15]
            },
            "scenarios_total": {
              "type": "integer",
              "description": "Total number of EPF scenarios defined in this feature (count of items in implementation.scenarios array). This is the denominator for test coverage %.",
              "$comment": "scenarios_total is ground truth from EPF feature definition. Tools read from count(implementation.scenarios). Used for: test coverage % (tested/total * 100), shipment decisions (typically require 100% scenario coverage), QA planning (scenarios_total - scenarios_tested = tests still needed), reports (Feature X has 7/10 scenarios tested = 70%).",
              "examples": [10, 12, 18, 25]
            }
          }
        }
      }
    }
  }
}
