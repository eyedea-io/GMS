id: fd-010
name: Knowledge Base & Content Management
category: business
definition:
  overview: >
    Organizational knowledge is fragmented across scattered documents, tools, and communication channels with no central repository or lifecycle management. 
    Content sprawl results in duplicate documents (42% of companies report multiple versions of same doc), outdated information risks (65% of teams 
    have encountered critical decisions based on obsolete documentation), and discovery challenges (employees spend 1.8 hours/day searching for information).
    This feature provides comprehensive content lifecycle management transforming scattered documents into centralized knowledge base with editorial workflows,
    version control, quality metrics, and taxonomy-driven organization. Content lifecycle management moves documents through structured states 
    (Draft → Review → Published → Archived) with role-based permissions at each stage preventing premature publishing and content quality degradation.
    Editorial workflows route documents through required approval gates enabling organizational quality standards enforcement across departments.
    Version control with semantic versioning (1.0 → 1.1 minor update, 1.0 → 2.0 major revision) preserves complete document history enabling rollback,
    change tracking, and audit compliance. Content quality metrics track freshness (last updated date), completeness (required sections populated),
    accuracy (peer review scores), and engagement (view counts, helpful votes) surfacing content needing attention. Taxonomy management with hierarchical
    categories, tags, and custom metadata enables flexible classification supporting multiple organizational perspectives (by department, by project,
    by compliance requirement). Content health dashboards provide visibility into stale content (90+ days since update), orphaned docs (no category),
    and quality trends enabling proactive content governance. Search integration leverages semantic search (fd-001) finding content by meaning vs keywords
    and inline collaboration (fd-009) enabling contextual discussion on content improving through community input. This transforms organizational knowledge
    from scattered tribal knowledge into managed strategic asset with measurable quality, accessibility, and lifecycle compliance.

  jobs_to_be_done:
    - "When I create new documentation, I want clear templates and workflows, so I can produce consistent quality content without guessing structure."
    - "When content needs review, I want automatic routing to appropriate reviewers, so I can get feedback quickly without manually tracking down stakeholders."
    - "When I search for information, I want to find current authoritative content, so I can trust decisions made based on that knowledge."
    - "When content becomes outdated, I want automatic flagging and review triggers, so critical information stays current without manual monitoring."
    - "When I manage organizational knowledge, I want visibility into content health metrics, so I can identify gaps and prioritize improvement efforts."

  strategic_context:
    problem_space: >
      Organizations accumulate thousands of documents across tools (Google Docs, Confluence, Notion, Sharepoint, wikis) with no lifecycle management
      resulting in content sprawl, quality degradation, and knowledge loss. Research shows employees spend 1.8 hours/day (36% of workweek) searching
      for information with 44% unable to find what they need. Document duplication creates version confusion with 42% of companies reporting multiple
      conflicting versions. Outdated content creates compliance and decision-making risks - 65% of teams have made critical decisions based on obsolete
      documentation. Lack of editorial workflows means content published without review causing quality inconsistency across departments. No content
      ownership leads to orphaned docs (23% of organizational documents have no active maintainer) that decay over time. Missing taxonomy structure
      prevents effective navigation and discovery beyond full-text search. Content creators lack templates and guidelines resulting in inconsistent
      structure and completeness. Organizations cannot measure content health or identify knowledge gaps for strategic improvement.

    existing_alternatives: >
      Organizations currently use general document management systems (Confluence, Notion, Sharepoint, Google Drive) providing basic CRUD and search
      but lacking content lifecycle workflows. These tools treat all docs equally without distinguishing authoritative published content from drafts.
      Version history exists but lacks semantic versioning or change summaries. Editorial workflows require manual coordination via comments or Slack.
      Content quality assessment depends on individual judgment with no standardized metrics or automated staleness detection. Taxonomy management is
      ad-hoc through folder structure (rigid) or tags (unstructured) without hierarchical classification. Some companies build custom editorial workflows
      via integrations (Confluence + Jira for review tasks) but this requires technical setup and breaks down with process changes. Knowledge base
      platforms (Guru, Document360) provide better structure but are siloed tools adding another system. Critically, existing tools don't surface
      content health metrics or provide proactive governance capabilities making knowledge management reactive rather than strategic.

    value_hypothesis: >
      Organizations adopting comprehensive content lifecycle management will reduce information discovery time by 50% (from 1.8h to 0.9h/day per employee
      = 3.5 hours saved weekly), eliminate version confusion through authoritative published content with clear versioning, reduce outdated content
      risk by 80% through automated staleness detection and review triggers, improve content quality consistency through editorial workflows and templates,
      and gain strategic visibility into knowledge gaps through content health metrics. For 100-person organization, discovery time savings alone equals
      350 hours/week ($17,500 weekly at $50/hr) = $910k annual productivity gain. Reduced rework from obsolete information (estimated 5% of project time
      wasted) adds $260k annual value. Content governance preventing compliance issues (one regulatory violation $50k-$500k) provides significant risk
      mitigation. Total value: $1.17M+ annually for mid-size organization with measurable ROI within first quarter through discovery time reduction alone.

  capabilities:
    - id: cap-001
      name: Content Lifecycle State Management
      description: >
        Structured content states (Draft → Under Review → Published → Archived) with automatic status progression, role-based permissions at each stage,
        and visibility controls. Draft content visible only to author + collaborators enabling safe experimentation. Under Review content visible to
        designated reviewers with commenting and approval capabilities. Published content visible organization-wide marked as authoritative current version.
        Archived content hidden from default search but accessible via explicit filter preserving historical record. State transitions enforce quality gates
        - cannot publish without required approvals, cannot archive while being actively referenced. Status badges visually distinguish content stage
        throughout interface. Workflow automation triggers notifications when content enters review queue, approval granted, or publication occurs.
        Lifecycle analytics track average time-in-state identifying review bottlenecks (e.g., avg 12 days in review vs target 3 days).
    
    - id: cap-002
      name: Editorial Workflow & Approval Routing
      description: >
        Configurable editorial workflows routing content through required approval stages before publication. Workflow templates by content type
        (Technical Doc: Engineering Lead → VP Engineering approval; Policy Doc: Legal → Compliance → Executive approval; Process Doc: Department Head
        approval). Automatic reviewer assignment based on content metadata (document category, affected teams, compliance tags) eliminating manual
        stakeholder identification. Approval tracking with explicit approve/reject actions, required feedback on rejection, and approval expiry
        (approvals older than 30 days require re-review if content updated). Parallel vs sequential approval support (Legal + Compliance can review
        simultaneously for speed; Executive approval must follow department approval for hierarchy). Escalation rules for stalled reviews (if no
        response in 5 days, notify reviewer manager; at 10 days, assign alternate reviewer). Workflow metrics showing approval velocity by reviewer
        (Sarah: 1.2 days avg, John: 6.8 days avg) and rejection rate by content type (Policy docs: 35% rejected first submission, Process docs: 12%).
    
    - id: cap-003
      name: Version Control & Change Management
      description: >
        Semantic versioning for published content with automatic version incrementing on publication (Draft updates don't change version; publish creates
        1.0; minor edit becomes 1.1; major rewrite becomes 2.0). Complete version history with diff view showing exactly what changed between versions
        (added/removed/modified sections highlighted). Change summary required on publication (author must explain 'what changed and why' in 2-3 sentences)
        surfaced in version history and notifications. Point-in-time viewing allowing users to see document as it existed at specific date (crucial for
        audit compliance and historical context). Rollback capability enabling reversion to previous version if current version proves problematic
        (rollback creates new version preserving complete history). Version comparison across non-sequential versions (compare 1.2 to 2.5 to see cumulative
        changes). Version analytics tracking major vs minor updates, publication frequency, and rollback incidents identifying unstable high-churn content.
    
    - id: cap-004
      name: Content Quality & Freshness Monitoring
      description: >
        Automated content health scoring across multiple dimensions with proactive staleness detection and quality alerts. Freshness tracking marks content
        stale after configurable period (90 days default, varies by content type: compliance docs 30 days, evergreen docs 180 days). Completeness scoring
        checks required sections populated based on content template (Technical ADR: Context/Decision/Consequences must exist; User Guide: each feature
        must have usage instructions). Accuracy assessment through peer review scores and helpful/unhelpful voting enabling community quality feedback.
        Engagement metrics track view count, unique viewers, search appearance rank, and average time-on-page identifying high-value vs unused content.
        Broken link detection scans published content for dead internal/external references (check monthly, surface in quality dashboard). Content health
        dashboard showing stale count by category (Engineering: 12 stale docs, Product: 5 stale), orphaned content (23 docs with no assigned owner),
        and quality trends (completeness score improved from 65% to 78% this quarter). Automatic review reminders sent to content owners when staleness
        threshold reached (90-day mark: gentle reminder; 120-day mark: urgent notification + manager copy; 150-day mark: flag for archival).
    
    - id: cap-005
      name: Taxonomy & Metadata Management
      description: >
        Hierarchical category structure with multi-dimensional tagging and custom metadata supporting flexible content classification and discovery.
        Category hierarchy up to 5 levels deep (Company > Engineering > Backend > API Design > REST Patterns) with inheritance (document in REST Patterns
        automatically tagged with all parent categories for broad discoverability). Tag system with controlled vocabulary (admins define official tags;
        users can suggest new tags requiring approval) preventing tag sprawl while enabling organic classification. Custom metadata fields by content type
        (Technical Docs have: technology_stack, affected_systems, deprecation_date; HR Docs have: policy_effective_date, applicable_locations, employee_types).
        Required vs optional metadata enforcement (all published content must have owner, category, and last_reviewed_date; other fields optional).
        Taxonomy analytics showing content distribution by category (Engineering: 245 docs, HR: 89 docs, Sales: 34 docs) highlighting knowledge concentration.
        Bulk recategorization tools enabling taxonomy refactoring (move all docs from old category to new structure) when organizational structure changes.
        Related content recommendations based on taxonomy overlap (docs sharing 3+ tags, same category, or linked by author) surfacing knowledge connections.

  personas:
    - id: content-creator
      name: Content Creator
      role: Documentation Contributor
      description: >
        Individual contributor creating documentation, guides, and process docs as part of their primary role (engineering, product, operations).
        Creates 2-4 documents per month. Values clear templates, easy workflows, and avoiding bureaucratic approval processes.
      goals:
        - "Create high-quality documentation without spending excessive time on formatting or structure."
        - "Get timely feedback from appropriate reviewers without manually chasing people down."
        - "Ensure my content stays current and relevant without constant manual monitoring."
        - "Make my documentation discoverable so time invested provides ongoing value to organization."
      pain_points:
        - "Unsure what structure to use for different document types resulting in inconsistent quality."
        - "Manual coordination to get reviews and approvals adding 1-2 weeks to publication process."
        - "No way to know if my documentation is being used or if it's become outdated and irrelevant."
        - "Fear that I'll be blamed if someone makes wrong decision based on my outdated doc that I forgot to update."
      usage_context: >
        Creates documentation 2-4 times per month as part of project completion, process improvement, or knowledge sharing. Needs streamlined
        workflows minimizing overhead. Prefers templates and guided structure over blank page. Values quick approvals (3-5 days max) over
        extensive review processes. Wants visibility into document usage and health.
      technical_proficiency: intermediate
      current_situation: "As a Senior Backend Engineer creating technical documentation as part of project delivery (API design docs, architecture decision records, runbooks - 2-3 docs per project, 8-12 docs annually), I struggle with documentation overhead adding non-value time to already packed project schedules. When I need to document an architecture decision, I start with blank Google Doc with no template or structure guidance. I look at previous ADRs to remember format (Context / Decision / Consequences / Alternatives Considered?) but different engineers use different structures so I'm never confident I'm doing it right. I spend 45 minutes just figuring out structure before writing actual content. Once doc is drafted, I need review from Tech Lead and Staff Engineer before sharing with broader team. I manually send Slack DMs ('Hey can you review this ADR when you get a chance?'), wait 3-5 days with no response, then send follow-up ('Any chance you can look at that ADR? Need to move forward on implementation'). Sometimes reviewers miss context because they don't realize it's blocking project progress - to them it's just another doc review request in sea of Slack messages. After finally getting approvals (usually 8-12 days), I share doc link in engineering Slack channel with 'FYI: new ADR on EventStoreDB selection'. Six months later, another team is making similar technology decisions and I wonder if they found my ADR or if they're starting from scratch. I have no idea if anyone reads my docs - maybe I'm wasting time on documentation no one uses. When technology decisions change (we migrated off EventStoreDB after 9 months due to operational complexity), my ADR is now misleading but I didn't update it because I don't have time to audit all past docs for currency. Someone might make decision based on obsolete ADR and I'd feel responsible for misleading them but I also can't spend 20% of my time maintaining old documentation."
      transformation_moment: "When I started using content lifecycle management with templates and editorial workflows, the documentation experience transformed from frustrating overhead to streamlined value creation. The first ADR I created using the Technical Decision Record template took 15 minutes instead of previous 60+ minutes - template provided clear sections (Context: What problem are we solving? Decision: What did we choose? Consequences: What are the implications? Alternatives: What else did we consider?) with inline guidance for each section ('Context should explain problem for engineer 6 months from now who wasn't in the room'). I filled sections with relevant information feeling confident in structure quality. When I marked doc Under Review, the system automatically identified required reviewers based on metadata (I tagged doc with backend, architecture, and data-storage tags → system assigned Tech Lead Sarah and Staff Engineer Mike as reviewers + added notification to #engineering-adrs Slack channel). Sarah and Mike each received notification with context ('Nikolai needs review on EventStoreDB ADR - blocking project scheduled to start Monday') and one-click approve/comment actions. Sarah reviewed and approved within 4 hours leaving inline comment on one section ('Great analysis - consider adding note about operational learning curve since that was our main challenge'). I incorporated feedback and Mike approved next day. Total review time: 36 hours vs previous 8-12 days. After publishing, the content health dashboard showed my ADR getting 45 views in first month (more engagement than I expected - clearly filling knowledge gap). At 6-month mark when we decided to migrate off EventStoreDB, I received automatic staleness alert ('EventStoreDB ADR has major system changes - review for currency'). I added Major Update section documenting migration decision and lessons learned ('Why we chose it initially: correct technical choice for use case. Why we migrated: underestimated operational complexity and team learning curve. Lessons: factor operations expertise into architecture decisions.'). This preserved valuable institutional memory - now when future teams consider EventStoreDB, they see both original reasoning AND outcome preventing repeated mistakes. The version history showed 127 total views with 8 views specifically of 'why we migrated' section proving value of maintenance effort."
      emotional_resolution: "I now feel confident creating documentation as streamlined value-add activity rather than frustrating bureaucratic overhead. Templates eliminate blank-page anxiety and structure uncertainty - I spend time on content substance rather than formatting decisions. Editorial workflows remove coordination burden - I mark doc Under Review and appropriate people are automatically notified with context and urgency rather than me manually chasing reviewers via Slack. Review turnaround improved from 8-12 days to 1-3 days because reviewers receive prioritized notifications with blocking context vs buried in general Slack messages. Usage analytics show my documentation is being consumed (typical ADR gets 30-50 views, popular ones get 100+) validating time investment and showing organizational impact. Automatic staleness alerts mean I maintain critical docs without manually auditing old content - system tells me when major changes warrant updates preventing documentation drift. Version history with update summaries creates valuable institutional memory - when we pivot away from technical decisions, I document why providing future teams with full context (initial reasoning + outcome + lessons learned) preventing repeated mistakes. Most importantly, I trust documentation system supports good practice rather than fighting against it - templates guide quality, workflows enforce review, staleness detection maintains currency, analytics show impact. Documentation became professional contribution to organizational knowledge rather than check-box compliance activity I resent."

    - id: content-editor
      name: Content Editor
      role: Knowledge Manager
      description: >
        Dedicated or part-time role responsible for organizational knowledge quality, editorial standards, taxonomy management, and content governance.
        Reviews 10-20 documents per week. Manages content lifecycle policies and quality initiatives.
      goals:
        - "Maintain consistent quality standards across all organizational documentation."
        - "Ensure content stays current and accurate through proactive governance rather than reactive firefighting."
        - "Develop effective taxonomy enabling intuitive navigation and discovery."
        - "Identify knowledge gaps and drive content creation where organizational needs exist."
      pain_points:
        - "No visibility into overall content health requiring manual audits to find stale or low-quality docs."
        - "Inconsistent quality across departments with no leverage to enforce editorial standards."
        - "Taxonomy becomes organically messy over time with no tools for structured refactoring."
        - "Cannot measure impact of knowledge management initiatives making it hard to justify investment."
      usage_context: >
        Reviews content daily as primary responsibility or 4-8 hours per week as part of broader ops/enablement role. Needs aggregate analytics
        and bulk management tools vs document-by-document workflows. Values proactive alerts (content becoming stale) over reactive discovery
        (finding problems during audit). Requires authority to enforce standards and drive improvements.
      technical_proficiency: intermediate
      current_situation: "As a Knowledge Manager for 200-person product organization (hired 6 months ago after executive team realized documentation chaos was slowing every team), I'm responsible for organizational knowledge quality but lack tools and leverage for effective governance. My mandate is to 'fix the documentation problem' but I inherited 2,400+ documents across Confluence (primary knowledge base), Google Docs (draft collaboration), Notion (some teams prefer it), and Sharepoint (legacy compliance docs). I have no visibility into overall content health - to find stale or problematic docs, I manually audit random samples spending 6-8 hours weekly reviewing 30-40 documents. This reactive approach means I discover problems months after they've caused issues. Last month I found ADR on microservices deployment strategy marked March 2022 (18 months old) that was factually obsolete (we'd completely changed infrastructure stack) but still ranking #2 in search results for 'deployment'. Three teams had referenced that obsolete ADR in recent planning causing 2 weeks of rework when they discovered current approach was different. I tried to establish editorial review process requiring all docs go through me before publication but quickly became bottleneck (20 docs per week submitted vs my capacity of 12) and engineers started bypassing the process publishing directly. I have no leverage to enforce standards because I can't monitor compliance or identify violations. Taxonomy is organically messy - we have 847 tags (most used by 1-2 docs), inconsistent category structure (Engineering has 8 subcategories, Sales has 0), and duplicate concepts (is it 'API Design' or 'API Architecture' or 'API Standards'?). I tried to refactor taxonomy but there are no bulk recategorization tools so I'd need to manually update hundreds of docs which is infeasible. I cannot measure impact of my knowledge management work - I improved some doc templates and created review guidelines but I have no metrics showing whether this resulted in better documentation quality or easier discovery. When executives ask 'are we getting ROI on knowledge management investment?', I have anecdotal stories but no data making it hard to justify my role or get resources for improvement initiatives."
      transformation_moment: "When content lifecycle management with quality monitoring and governance tools was implemented, I finally gained visibility and leverage to drive organizational knowledge improvement. The content health dashboard transformed my reactive manual audits into proactive data-driven governance. First login showed aggregate metrics I'd never had: 487 stale docs (not updated in 90+ days = 20% of knowledge base), 156 orphaned docs (no assigned owner), 89 docs missing required metadata, completeness score 61% (many docs missing critical sections from templates). This immediately prioritized my work - those 487 stale docs were highest risk. I filtered to Engineering department (largest content volume) and sorted by view count to identify high-impact stale docs. Top item was 'REST API Design Guide' with 340 views in last 90 days but not updated in 14 months. I assigned review to Tech Lead Sarah with automated reminder ('High-traffic content stale - review for currency by Friday'). She updated doc within 2 days because targeted assignment with specific deadline was clear actionable task vs vague 'someone should update docs sometime'. I set up automatic staleness workflows eliminating my manual intervention: at 90 days, automatic reminder to content owner; at 120 days, escalate to owner's manager; at 150 days, flag for archival with last-chance notification. Over 8 weeks, stale doc count dropped from 487 to 89 (82% reduction) with only 6 hours of my time spent on escalations vs previous 24+ hours weekly on manual audits. The taxonomy refactoring tools enabled systematic cleanup I'd been wanting to do for months - I merged duplicate concepts in bulk ('API Design' + 'API Architecture' + 'API Standards' → 'API Standards' moving all 47 docs in single operation), restructured Engineering categories to match new org structure (5-team reorg required moving 120 docs - completed in 30 minutes vs days of manual work), and deprecated 200+ orphan tags (used by only 1 doc each). After refactoring, I ran related content analysis discovering knowledge gaps: Security had 12 docs vs Engineering's 245 docs despite being critical domain, 34 projects had no associated documentation showing process compliance issues, Onboarding category had docs for Engineering and Product but nothing for Sales/Support/Operations. I presented these gaps to leadership with specific targets ('Create 15 Security docs in Q2 covering critical domains; establish doc-per-project policy with enforcement; develop role-based onboarding docs for non-technical teams'). This data-driven approach got immediate buy-in and budget approval because gaps were objectively demonstrated not just my opinion."
      emotional_resolution: "I now feel effective as Knowledge Manager with clear visibility into organizational knowledge health and leverage to drive improvement rather than fighting documentation chaos reactively. The content health dashboard gives me comprehensive situational awareness - I know exactly how many stale docs exist, which categories have quality issues, where knowledge gaps exist, and what trends are improving vs deteriorating. This transformed my work from reactive firefighting (discover stale doc during manual audit → scramble to get updated) to proactive governance (system identifies staleness automatically → I assign targeted review with context → automated workflows handle follow-up and escalation). Automated staleness workflows handle the busywork I used to do manually - reminding content owners, escalating to managers, flagging for archival - freeing 20+ hours per month for strategic knowledge initiatives. Taxonomy refactoring tools enabled systematic cleanup I couldn't do before - merging duplicates, restructuring hierarchies, deprecating orphan tags - giving organization coherent navigable structure supporting intuitive discovery. Content quality analytics show objective impact of my work: average completeness score improved from 61% to 84% in 6 months, stale content reduced from 20% to 4% of knowledge base, review velocity improved from 8.2 days average to 2.1 days with editorial workflows. When executives ask about ROI, I present concrete metrics: discovery time improved 35% (employee survey before/after), rework from obsolete docs reduced 70% (incident tracking), knowledge base engagement up 40% (view counts and search metrics). This evidence-based approach secured additional resources - I hired junior Knowledge Coordinator and got budget for advanced taxonomy consulting - because leadership sees measurable value. Most importantly, I shifted perception of knowledge management from bureaucratic overhead to strategic enabler - teams now see editorial workflows and quality standards as supporting their work (faster reviews, better discoverability, reduced rework from stale info) rather than hindering it."

    - id: subject-matter-expert
      name: Subject Matter Expert
      role: Technical Reviewer
      description: >
        Senior individual contributor (Staff Engineer, Principal PM, Senior Specialist) with deep domain expertise called upon to review technical
        accuracy and strategic alignment of documentation. Reviews 5-10 documents per month as secondary responsibility alongside primary work.
      goals:
        - "Ensure technical accuracy of documentation in my domain preventing misleading or outdated information."
        - "Provide valuable feedback efficiently without review responsibilities becoming overwhelming burden."
        - "Contribute to organizational knowledge without formal documentation creation responsibility."
        - "Maintain awareness of how organizational understanding of my domain is documented and communicated."
      pain_points:
        - "Review requests arrive via ad-hoc Slack messages making it hard to prioritize or track what needs review."
        - "Lack of context about why review is needed or urgency making it hard to schedule appropriately."
        - "Cannot easily track what I've reviewed before when similar content needs re-review after updates."
        - "No visibility into whether my feedback was incorporated or if further discussion is needed."
      usage_context: >
        Reviews content 5-10 times per month as needed rather than scheduled workflow. Needs clear prioritization (which reviews are blocking?),
        sufficient context (what changed? why does this need my eyes?), and feedback tracking (was my input addressed?). Values efficiency -
        reviewing should take 15-30 minutes not require deep investigation.
      technical_proficiency: expert
      current_situation: "As a Staff Engineer specializing in distributed systems architecture (8 years at company, recognized technical expert in service mesh, event-driven patterns, and reliability engineering), I'm frequently asked to review technical documentation to ensure accuracy and alignment with architectural standards but the review process is chaotic overhead on top of my already packed primary responsibilities. Review requests arrive via direct Slack messages from engineers I may or may not know personally ('Hey, could you review this ADR on service communication patterns when you get a chance?') with Google Doc link and no additional context. I have 8-15 such messages per month mixed in with other Slack conversations. Without urgency information, I default to treating all reviews as low-priority (I'm already at 120% capacity with architecture initiatives) so review requests sit in my todo list for 4-8 days before I get to them. Sometimes I discover after finally reviewing that the doc was blocking project kickoff and team has been waiting on me - I feel bad about delay but I had no way to know it was urgent from 'when you get a chance' message. When I open Google Doc to review, I often lack context to provide meaningful feedback efficiently - if doc is update to existing content, I don't know what changed so I either review entire doc from scratch (inefficient) or make assumptions about what's new (risky - might miss important changes). If doc references our existing architecture, I don't have easy access to related documentation to verify consistency. Last month I reviewed API versioning strategy doc that seemed reasonable in isolation but contradicted guidelines in our REST Standards doc I'd reviewed 6 months earlier - I only caught inconsistency because I remembered that prior doc, but no system helped me identify the conflict. After providing feedback via Google Doc comments, I have no tracking of whether feedback was addressed - doc disappears from my awareness and I assume author incorporated my input. Three weeks ago I discovered published documentation containing approach I'd specifically recommended against in review comments - author had published without addressing my feedback and I didn't know because there was no closure workflow. I feel responsible for technical accuracy of organizational documentation (people make architectural decisions based on these docs) but I lack tools to fulfill that responsibility effectively - I can't prioritize reviews appropriately, can't track my past contributions to related content, and can't verify my feedback is incorporated before publication."
      transformation_moment: "When editorial workflows with reviewer assignment and feedback tracking were implemented, my review experience transformed from chaotic ad-hoc requests to structured trackable process. Review requests now appear in centralized Review Queue showing all pending reviews with sortable columns: Document Name, Requester, Urgency (Blocking/High/Normal), Submission Date, Days Waiting. First time I opened queue, I saw 6 pending reviews and immediately understood prioritization: 2 marked Blocking (project kickoffs waiting), 3 marked Normal (background documentation), 1 marked Low (exploratory research doc). I completed the 2 Blocking reviews within 24 hours unblocking projects, then tackled Normal reviews over next few days based on how long they'd been waiting. Average review turnaround improved from 6 days (my previous unstructured approach) to 2 days (structured prioritization). The context provided with each review transformed my efficiency - when I opened 'GraphQL Gateway Strategy' ADR, the review request showed: 'Major update - Changed from Apollo Federation to Custom Gateway solution. See sections 3.2 and 4.1 for key changes. Blocking Q2 Architecture Initiative.' I immediately focused on sections 3.2 and 4.1 seeing exactly what changed vs wasting time re-reading entire doc. Related documentation was automatically surfaced ('This doc references: API Standards 2.3, Event-Driven Patterns 1.4, Service Mesh Guide 2.1') with one-click access. I caught inconsistency with Service Mesh Guide (doc recommended service-to-service auth that conflicted with our mesh-handled auth approach) and left comment: 'Conflicts with Service Mesh Guide 2.1 section on auth. If intentional deviation, add rationale; if oversight, align with mesh approach.' The feedback tracking meant I knew exactly when my comment was addressed - I received notification 'Author responded to your feedback on GraphQL Gateway Strategy ADR' with author's reply: 'Good catch - updated section 3.2 to align with mesh auth. Can you re-review for approval?' I re-reviewed updated section in 5 minutes and approved. This closed-loop process ensured my feedback was incorporated before publication vs previous uncertainty. The review analytics dashboard showed my contribution impact: 47 docs reviewed in 6 months, average review time 1.8 days (down from 6.2 days pre-system), 89% of my feedback incorporated before publication. During performance review, my manager specifically highlighted review contribution using these metrics: 'Your expert reviews improved documentation quality and unblocked 18 project kickoffs that were waiting on technical validation.'"
      emotional_resolution: "I now feel effective as technical reviewer with clear prioritization, sufficient context, and feedback tracking ensuring my domain expertise contributes to organizational knowledge quality without overwhelming my primary responsibilities. The centralized review queue with urgency indicators lets me prioritize appropriately - I handle blocking reviews immediately (unblocking projects within 24 hours) and schedule normal reviews in my weekly workflow (Friday afternoons for documentation tasks) vs previous chaos where everything was invisible low-priority. Context provided with each review (what changed? why does this need review? what's the urgency?) enables efficient focused review - I spend 15-20 minutes per review vs previous 40+ minutes trying to understand scope and related docs. Related documentation surfacing helps me verify consistency with existing standards - system shows me potentially conflicting docs rather than relying on my memory of everything I've reviewed in past 2 years. Feedback tracking with closed-loop workflow ensures my input is incorporated before publication - I receive notifications when authors respond to my comments and can verify changes before approving vs previous uncertainty where docs might publish without addressing my concerns. Review analytics showing my contribution (47 reviews, 1.8 day average turnaround, 89% feedback incorporation rate) provide concrete evidence of impact for performance reviews and recognition. Most importantly, I trust the review system supports my domain expertise responsibility effectively - reviews are prioritized appropriately, sufficient context is provided, my feedback is tracked to closure, and I can see impact of my contributions. I no longer feel like review requests are chaotic burden disrupting my work - they're structured manageable workflow integrated into my weekly routine where I contribute 3-4 hours weekly to organizational knowledge quality as valued secondary responsibility alongside my primary architecture initiatives."

    - id: knowledge-seeker
      name: Knowledge Seeker
      role: Information Consumer
      description: >
        Any employee searching for documentation to inform their work. Searches knowledge base 3-8 times per week. Values finding current authoritative
        information quickly without wading through outdated or irrelevant content.
      goals:
        - "Find relevant current documentation quickly without extensive search refinement."
        - "Trust that information found is accurate and current enabling confident decision-making."
        - "Discover related content and knowledge connections expanding understanding beyond initial search."
        - "Provide feedback on content quality helping organizational knowledge improvement."
      pain_points:
        - "Search returns large result sets with no way to distinguish authoritative current content from outdated drafts."
        - "Cannot tell if documentation is current or obsolete without manually checking metadata or asking colleagues."
        - "After finding initial doc, cannot easily discover related content requiring repeated searches."
        - "No way to provide feedback when documentation is incomplete, inaccurate, or outdated."
      usage_context: >
        Searches knowledge base 3-8 times per week as needed for project work, decision-making, or learning. Needs fast relevant results,
        clear currency indicators, and easy navigation to related content. Values trustworthy authoritative sources over comprehensive results.
      technical_proficiency: intermediate
      current_situation: "As a Product Manager building roadmap for payment processing features (2 years at company, working on FinTech product serving 5,000+ merchants), I search internal documentation 5-8 times weekly to understand technical architecture, compliance requirements, and past product decisions but search experience is frustrating resulting in wasted time and uncertainty about information currency. When I search for 'PCI compliance requirements', I get 23 results spanning 3 years with no way to distinguish current authoritative content from obsolete drafts or deprecated approaches. Result titles are similar ('PCI Compliance Guide', 'PCI DSS Requirements', 'Payment Security Standards') with no indicators of which is official current version. I open 4-5 docs reading intros trying to determine which is most relevant and current. I find two docs with contradictory information about data retention (one says 'store last 4 digits only', another says 'full card data allowed with encryption and annual audit') - I have no way to know which is correct without asking Security team via Slack. Sometimes I discover after making decisions that I used outdated documentation - last quarter I designed feature based on 'REST API Standards' doc that turned out to be 18 months obsolete (we'd moved to GraphQL-first approach). I wasted 2 weeks of design work because search didn't indicate doc was stale. After finding relevant documentation, I struggle to discover related content that would expand my understanding - if I read 'Payment Gateway Integration' doc, there are probably related docs on error handling, testing, and compliance implications but I have to perform separate searches for each concept rather than surfacing connections automatically. When I find documentation that's incomplete or inaccurate (e.g., missing information about international payment flows or contradicting what Engineering told me in meeting), I have no way to provide feedback - I either send Slack message to doc author if I happen to know who wrote it, or I just note the issue for myself and move on leaving the problem for next person. This creates institutional knowledge gaps where everyone encounters same doc problems but nobody fixes them because feedback mechanism doesn't exist."
      transformation_moment: "When knowledge base search with quality indicators and related content surfacing was implemented, my documentation experience transformed from frustrating uncertainty to confident efficient discovery. First search for 'PCI compliance requirements' showed 23 results but now with clear indicators of content quality and currency: top result was 'PCI DSS Compliance Guide' with green Published badge, freshness indicator (Updated 12 days ago), authoritativeness signal (Official - Security Team), view count (342 views this month suggesting high trust), and quality score (92/100 based on completeness, accuracy votes, and review status). I immediately recognized this as authoritative current content vs having to open multiple docs to determine relevance. Result previews showed first paragraph giving enough context to confirm relevance before clicking ('This guide covers PCI DSS Level 1 requirements for payment processors handling 6M+ transactions annually. Last updated November 2023 reflecting PCI DSS v4.0 compliance deadline...'). When I opened doc, I saw currency confidence indicators: Last Reviewed: 12 days ago by Security Team, Next Review: February 2024, Version: 2.3 (Major update in Nov 2023 for PCI DSS v4.0), Edit History showing 8 updates over 18 months (actively maintained not stagnant). This gave me confidence to base product decisions on this information vs uncertainty about whether it was current. The related content panel transformed discovery - instead of performing separate searches for connected concepts, system automatically surfaced: Related by Category (other Security docs), Related by Tags (docs also tagged with compliance, payments, data-security), Referenced By (4 docs linking to this guide suggesting downstream usage), Frequently Viewed Together (85% of people who read this also read 'Payment Gateway Error Handling'). I discovered 'Payment Gateway Error Handling' doc I didn't know existed that was directly relevant to my feature design - this cross-discovery happened automatically rather than requiring me to guess right search keywords. When I found section on international payments that seemed incomplete (covered EU but not APAC region we're expanding into), I clicked 'Provide Feedback' leaving inline comment: 'Missing APAC region guidance - we're expanding to Singapore and Australia in Q2, need compliance requirements for those markets.' Two days later I received notification: 'Security Team responded to your feedback on PCI Compliance Guide' with response: 'Good catch - we're updating APAC section this week as part of Singapore launch prep. You'll get notified when published.' This closed-loop feedback meant my input contributed to organizational knowledge improvement rather than disappearing into void."
      emotional_resolution: "I now feel confident finding accurate current information efficiently enabling me to make product decisions based on trustworthy documentation rather than hunting through obsolete content or constantly verifying with colleagues. Search quality indicators (freshness, authoritativeness, view counts, quality scores) immediately distinguish authoritative current content from outdated drafts - I trust top results are actually most relevant vs having to open multiple docs to determine quality manually. Result previews give enough context to confirm relevance before clicking eliminating wasted time opening irrelevant docs. Currency confidence indicators (last reviewed date, next review date, version with change summary, edit history) give me clear signal about whether content is actively maintained and current - I know if information reflects latest organizational knowledge or if I should verify elsewhere. Related content surfacing enables serendipitous discovery of relevant docs I didn't know to search for - I consistently discover 2-3 additional relevant docs per search through 'Frequently Viewed Together' and 'Referenced By' suggestions vs previous approach where I only found what I explicitly searched for. Feedback mechanism gives me voice in knowledge quality improvement - when I find incomplete or inaccurate content, I can leave targeted feedback rather than just noting issues personally or sending scattered Slack messages. Getting notifications when my feedback is addressed creates satisfying closed-loop contribution to organizational knowledge. The search experience shifted from frustrating time-sink (15-25 minutes to find and verify information currency through multiple doc review + Slack verification) to efficient confident discovery (3-5 minutes to find authoritative current content with high confidence in quality). This freed 8-12 hours monthly previously spent hunting and verifying information enabling me to focus on actual product strategy work rather than documentation archaeology."

  scenarios:
  - id: scn-001
    name: Content Creator publishes technical guide using template and editorial workflow
    actor: content-creator
    context: ctx-001
    trigger: "Senior Backend Engineer completing EventStoreDB evaluation project needs to document decision and implementation guidance for future teams considering similar event-sourcing solutions."
    action: >
      1. Sarah navigates to Content Library and clicks "Create New Document" selecting "Technical Decision Record (ADR)" template from content type dropdown.
      2. System generates new draft with structured sections pre-populated (Context, Decision, Consequences, Alternatives Considered, Implementation Notes) and inline guidance for each section.
      3. Sarah fills Context section (250 words) explaining event-sourcing need for audit trail, fills Decision section (200 words) documenting EventStoreDB selection with cost/capability analysis, fills Consequences section (300 words) covering operational implications, and fills Alternatives (150 words) comparing Kafka and PostgreSQL.
      4. She adds metadata: Category = Architecture/Data-Storage, Tags = event-sourcing + event-store + audit, Affected Systems = payment-service + audit-service, Owner = sarah@company.com.
      5. After 40 minutes of writing, she clicks "Submit for Review" changing doc status from Draft to Under Review. System automatically identifies required reviewers based on metadata (Architecture tag → Tech Lead Mike, Data-Storage category → Staff Engineer Lisa) and sends Slack notifications with review request.
      6. Mike and Lisa each receive notification: "Sarah needs review on EventStoreDB ADR - Blocking Q3 Initiative, Review by Friday" with one-click "Open Document" and "I'll Review This" acknowledgment buttons.
      7. Mike reviews within 8 hours leaving inline comment on Consequences section: "Add note about operational learning curve - this was our main challenge." Sarah incorporates feedback same day.
      8. Lisa reviews next morning approving with comment: "Solid analysis. Might also mention event schema versioning implications for Alternatives section." Sarah adds paragraph on schema versioning.
      9. After both approvals received (36 hours total review time), Sarah clicks "Publish". System increments version to 1.0, marks status Published, posts announcement to #engineering-adrs Slack channel: "New ADR: EventStoreDB Selection (sarah@company.com) - event-sourcing, audit, data-storage".
      10. Published doc appears in search results with green Published badge, displays view count tracking (45 views in first month), and enters freshness monitoring (will trigger staleness alert at 90 days if not reviewed).
    outcome: "Sarah created high-quality technical documentation in 2 hours total time (40 min writing + 15 min feedback incorporation + concurrent 36-hour review wait) vs previous 60+ min writing + 8-12 day review coordination. Template eliminated structure uncertainty and guided content quality. Editorial workflow automatically routed to appropriate reviewers with context and urgency eliminating manual Slack coordination. Fast review turnaround (36 hours vs previous 8-12 days) meant doc published while project context was fresh. Published doc serves as authoritative reference for future teams considering event-sourcing (45 views in month 1, 127 total views over 6 months). Six months later when team migrated off EventStoreDB, automatic staleness alert prompted Sarah to add Major Update section documenting migration reasoning and lessons learned preserving valuable institutional memory for future architectural decisions."
    acceptance_criteria:
      - "Draft document created from template with pre-structured sections and inline guidance in under 2 minutes."
      - "Metadata entry (category, tags, affected systems, owner) completed via dropdown and autocomplete in under 3 minutes."
      - "Submit for Review automatically identifies 2 required reviewers based on metadata and sends Slack notifications with 'Review by [date]' deadline."
      - "Reviewers can view document, leave inline comments, and approve/reject with one-click actions from review interface."
      - "After all required approvals received, Publish action increments version to 1.0, changes status to Published, and posts announcement to configured Slack channel."
      - "Published document appears in search results with Published badge, freshness indicator (Updated X days ago), and view count tracking."
      - "At 90 days after publication, system sends automatic staleness reminder to document owner: 'EventStoreDB ADR approaching staleness threshold - review for currency'."
  
  - id: scn-002
    name: Knowledge Manager identifies and remediates stale high-traffic content using health dashboard
    actor: content-editor
    context: ctx-002
    trigger: "Weekly content governance review where Knowledge Manager assesses organizational knowledge health and prioritizes improvement actions."
    action: >
      1. Maria opens Content Health Dashboard showing aggregate metrics: 487 stale docs (20% of 2,400 doc knowledge base), 156 orphaned docs, 89 docs missing required metadata, overall completeness score 61%.
      2. She filters to Engineering department (largest content volume with 890 docs) and sorts Stale Docs by View Count descending to identify highest-impact stale content needing immediate attention.
      3. Top result is "REST API Design Guide" with 340 views in last 90 days but Last Updated: 14 months ago (May 2022), Status: Published, Owner: john@company.com (Staff Engineer who left company 6 months ago indicating ownership issue).
      4. Maria clicks into doc seeing quality flags: Broken Links: 3 detected (external references to deprecated tools), Completeness: 78% (missing sections on versioning and error codes per template), Review Status: No review in 14 months.
      5. She reassigns ownership from departed john@company.com to current Tech Lead sarah@company.com with note: "High-traffic doc (340 views/90 days) needs currency review and broken link fixes. Priority: High."
      6. She sets custom staleness threshold for this doc (30 days vs default 90 days) given high traffic and rapid API evolution, and schedules quarterly review recurrence.
      7. Sarah receives notification: "You've been assigned ownership of REST API Design Guide - High-traffic content needs review (340 views recently, 14 months since update). Review by Friday." with one-click "Start Review" action.
      8. Sarah reviews doc over 2 days: fixes 3 broken links to updated tool versions, adds missing Versioning section (200 words on semantic versioning and deprecation policy), adds Error Codes section (300 words with standard code catalog), and updates Authentication section to reflect current OAuth 2.1 implementation.
      9. She publishes updates with change summary: "Major update for currency: fixed broken links, added versioning guidance, added error code standards, updated auth for OAuth 2.1. All sections now reflect current practices as of Dec 2023." System increments version from 1.4 to 2.0 (major update).
      10. Maria's dashboard updates in real-time: Engineering stale docs decreased from 487 to 486, REST API Design Guide moved from Stale to Healthy with green indicator, completeness score improved from 61% to 62% (one high-impact doc fixed).
      11. Over 8 weeks, Maria uses dashboard to systematically address top 50 stale high-traffic docs (similar triage → reassign → review → publish workflow). Engineering stale count drops from 487 to 89 (82% reduction), overall knowledge base completeness improves from 61% to 78%.
    outcome: "Maria transformed reactive manual documentation audits into proactive data-driven governance using content health visibility. She identified highest-impact stale content (high traffic + obsolete) in 5 minutes via sortable dashboard vs previous approach of random manual sampling discovering problems months after they cause issues. Targeted ownership reassignment with context and deadline enabled fast remediation - Sarah updated critical doc in 2 days because request was specific and urgent vs vague 'someone should update docs sometime' that never happens. Systematic application of triage workflow across 8 weeks reduced stale content by 82% (487 to 89 docs) with only 6 hours weekly of Maria's time vs previous 24+ hours on ineffective manual audits. Organizational documentation shifted from 20% stale (high risk of decisions based on obsolete info) to 4% stale (acceptable maintenance threshold). Most importantly, process became self-sustaining through automated staleness alerts and ownership accountability rather than relying on Knowledge Manager's heroic manual effort."
    acceptance_criteria:
      - "Content Health Dashboard displays aggregate metrics: total docs, stale doc count with percentage, orphaned doc count, completeness score percentage, filterable by department/category."
      - "Dashboard table shows all docs with sortable columns: Title, Status, Owner, Last Updated, View Count (90 days), Quality Score, with ability to filter by Status (Stale/Healthy/Orphaned) and sort by any column."
      - "Clicking doc row opens detail view showing: quality flags (broken links, missing sections, completeness percentage), edit history (last 10 updates with version and change summary), view analytics (views over time graph), and ownership info."
      - "Reassigning ownership generates notification to new owner with context: '{Doc title} assigned to you - High-traffic content needs review ({view count} views recently, {time} since update). Review by {deadline}.' with one-click 'Start Review' action."
      - "Setting custom staleness threshold overrides default 90-day setting and saves as doc metadata affecting future staleness calculations and alert scheduling."
      - "Publishing update with change summary increments version (minor update: 1.4→1.5, major update: 1.4→2.0 based on semantic versioning rules) and saves change summary as version metadata visible in version history."
      - "Dashboard metrics update in real-time as docs are remediated showing: stale count decrease, completeness score improvement, aggregate health trends over time (line graph showing last 90 days)."
  
  - id: scn-003
    name: Knowledge Seeker discovers authoritative current content through quality-enhanced search
    actor: knowledge-seeker
    context: ctx-003
    trigger: "Product Manager designing payment processing features searches for PCI compliance requirements to inform technical requirements and security controls."
    action: >
      1. Alex enters "PCI compliance requirements" in knowledge base search and presses Enter. System performs semantic search (leveraging fd-001 Knowledge Graph) finding 23 potentially relevant docs.
      2. Search results display with rich quality indicators: Top result "PCI DSS Compliance Guide" shows green Published badge, freshness "Updated 12 days ago", authority "Official - Security Team", view count "342 views this month", quality score "92/100", and 2-sentence preview: "This guide covers PCI DSS Level 1 requirements for payment processors handling 6M+ transactions annually. Last updated November 2023 reflecting PCI DSS v4.0 compliance deadline..."
      3. Second result "Payment Security Standards" shows yellow Under Review badge, "Last updated 4 months ago", "Draft - awaiting Security Team approval", 45 views this month, quality score 67/100 indicating less authoritative source.
      4. Third result "PCI Compliance Checklist (Legacy)" shows gray Archived badge, "Archived July 2023", "Superseded by PCI DSS Compliance Guide 2.0", quality score N/A indicating obsolete content.
      5. Alex clicks top result (PCI DSS Compliance Guide) confident it's most authoritative based on Published status, recent update, Security Team ownership, and high view count.
      6. Doc opens showing currency confidence indicators: Last Reviewed: 12 days ago by Security Team, Next Review: February 2024, Version: 2.3 (Major update Nov 2023 for PCI DSS v4.0), Edit History: 8 updates over 18 months.
      7. Related Content panel on right sidebar displays automatically: Related by Category (5 other Security docs), Related by Tags (docs tagged compliance + payments + data-security), Referenced By (4 docs linking to this guide), Frequently Viewed Together (85% of readers also viewed "Payment Gateway Error Handling", 72% viewed "Cardholder Data Encryption Guide").
      8. Alex reads PCI guide (15 min) then clicks "Payment Gateway Error Handling" from Frequently Viewed Together discovering relevant doc she didn't know to search for explicitly. This doc covers retry logic and failure handling critical to her feature design.
      9. While reading section on international payments, Alex notices incomplete coverage (EU covered but not APAC region company is expanding into). She clicks "Provide Feedback" button and enters inline comment: "Missing APAC region guidance - we're expanding to Singapore and Australia in Q2, need compliance requirements for those markets."
      10. Two days later Alex receives notification: "Security Team responded to your feedback on PCI Compliance Guide" with response: "Good catch - we're updating APAC section this week as part of Singapore launch prep. You'll get notified when published." One week later: "PCI Compliance Guide updated - your feedback addressed" linking to new APAC section.
    outcome: "Alex found authoritative current PCI compliance documentation in under 5 minutes (vs previous 20-25 minutes hunting through multiple docs and verifying currency via Slack) enabling confident product decision-making based on trustworthy information. Quality indicators (Published status, recent update, Security Team authority, high view count, 92/100 quality score) immediately distinguished authoritative guide from draft or archived content eliminating uncertainty about which doc to trust. Currency confidence indicators (last reviewed 12 days ago, next review scheduled, version 2.3 with change summary, active edit history) gave clear signal that content reflected latest organizational knowledge reducing need to verify with colleagues. Related content surfacing enabled discovery of relevant 'Payment Gateway Error Handling' doc she didn't know existed - this serendipitous discovery (85% of PCI guide readers also view error handling) happened automatically rather than requiring correct search keywords. Feedback mechanism gave Alex voice in knowledge quality improvement - her APAC coverage gap observation contributed to organizational knowledge completeness. Closed-loop notification when feedback addressed (7 days later) created satisfying contribution experience. Overall search experience shifted from 20-25 minute frustrated hunt-and-verify process to 5-minute confident discovery with high trust in information quality enabling Alex to focus on product strategy rather than documentation archaeology."
    acceptance_criteria:
      - "Search results display quality indicators for each result: Status badge (Published/Under Review/Archived with color coding), Freshness text (Updated X days/months ago), Authority signal (Official - [Team] or Draft - [Owner]), View count (X views this month), Quality score (0-100 based on completeness + accuracy + engagement)."
      - "Result previews show first 2 sentences of content giving sufficient context to assess relevance before clicking."
      - "Clicking result opens document with currency confidence indicators visible above content: Last Reviewed date with reviewer name/team, Next Review scheduled date, Version number with change summary on hover, Edit History link showing last 10 updates."
      - "Related Content panel displays automatically with 4 sections: Related by Category (up to 5 docs from same category), Related by Tags (docs sharing 2+ tags), Referenced By (docs linking to this one), Frequently Viewed Together (top 3 docs viewed by 60%+ of this doc's readers)."
      - "Provide Feedback button opens inline comment interface anchored to specific section allowing users to enter feedback text (50-500 chars) with optional urgency flag (Low/Medium/High)."
      - "Feedback submitted generates notification to document owner: '{User} provided feedback on {Doc}: {first 100 chars}' with one-click 'View Feedback' and 'Respond' actions."
      - "When owner responds to feedback or updates doc addressing feedback, original feedback provider receives notification: '{Owner} responded to your feedback on {Doc}' with link to response or updated section."

dependencies:
  requires:
    - fd-001  # Semantic search for intelligent content discovery beyond keyword matching
    - fd-007  # Organization hierarchy for permission management and ownership assignment
    - fd-009  # Inline collaboration for contextual discussion on content during review process
  enables:
    - fd-011  # Notification system depends on content lifecycle events (published, stale, review needed) as triggers
    - "Technical implementation features requiring documented API standards, architecture patterns, and process guidance"
    - "Compliance initiatives requiring documented policies, procedures, and evidence of organizational knowledge management"

boundaries:
  non_goals:
    - "Real-time collaborative editing (Google Docs-style) - focus is lifecycle management not simultaneous editing"
    - "External public documentation hosting - scope is internal organizational knowledge only"
    - "Automated content generation or AI writing - authors create content, system manages lifecycle and quality"
    - "Learning Management System (LMS) features like courses, quizzes, certifications - knowledge base not training platform"
  constraints:
    - "Content categorization hierarchy limited to 5 levels depth preventing overly complex taxonomy"
    - "Version history retained indefinitely for published content (compliance requirement) but may be archived to cold storage after 7 years"
    - "Editorial workflows support maximum 5 sequential approval stages (more stages indicate process dysfunction needing org change)"
    - "Search indexing has 5-minute lag for newly published content (acceptable trade-off for search performance)"
  edge_cases:
    - "Content with circular dependencies (Doc A references Doc B which references Doc A) - system allows but flags in quality dashboard"
    - "Mass content ownership transfer when employee departs - bulk reassignment tool supports up to 500 docs per operation"
    - "Taxonomy refactoring affecting hundreds of docs - category merge/split operations batched and reversible"
    - "Content published then immediately discovered to contain confidential info - emergency unpublish workflow bypasses normal archival process"

contexts:
  - id: ctx-001
    type: ui
    name: Content Creation & Editing Interface
    description: >
      Users access this interface when creating new documentation or editing existing drafts. Provides structured editor with template guidance,
      metadata entry, and submission workflow for routing content through editorial review before publication.
    key_interactions:
      - "Select content type from template library (Technical ADR, Process Guide, Policy Doc, User Guide, etc.) to create new document with pre-structured sections."
      - "Fill document sections using rich text editor with inline guidance hints showing what to include in each section."
      - "Add metadata via dropdowns and autocomplete: Category (hierarchical selection), Tags (controlled vocabulary with suggestions), Affected Systems (multi-select from org systems catalog), Owner (user picker)."
      - "Save as Draft (autosaves every 30 seconds) preserving work in progress visible only to author and explicit collaborators."
      - "Submit for Review changing document status and triggering automatic reviewer assignment based on metadata rules and notification to reviewers."
    data_displayed:
      - "Content type template selection showing: template name, description, typical use cases, required sections, and estimated completion time."
      - "Document editor showing: structured sections from template with inline guidance, formatting toolbar, character/word count, last saved timestamp."
      - "Metadata entry form showing: Category dropdown with hierarchical tree, Tags autocomplete with suggestion, Affected Systems multi-select, Owner user picker, Required vs Optional field indicators."
      - "Document status indicator (Draft/Under Review/Published/Archived) with color coding and status-appropriate actions (Save Draft, Submit for Review, Publish, Request Changes)."
      - "Real-time collaborator presence showing other users currently viewing or editing document (if inline collaboration fd-009 enabled)."

  - id: ctx-002
    type: ui
    name: Content Health Dashboard
    description: >
      Knowledge Managers and Content Editors access this dashboard for organizational knowledge governance. Shows aggregate content health metrics,
      identifies stale or problematic content, and provides bulk management tools for systematic knowledge base improvement.
    key_interactions:
      - "View aggregate metrics cards showing: total doc count, stale doc count with percentage, orphaned doc count, average completeness score, quality trend graph."
      - "Filter content by department, category, status (Stale/Healthy/Orphaned), owner, or date range to focus on specific knowledge base segments."
      - "Sort content table by any column (Title, Status, Owner, Last Updated, View Count, Quality Score) to identify priority improvement targets."
      - "Click document row to open detail view showing: quality flags, edit history, view analytics, ownership info, and available remediation actions."
      - "Bulk select documents using checkboxes to perform mass operations: reassign ownership, recategorize, trigger review reminders, or update staleness thresholds."
    data_displayed:
      - "Aggregate metrics cards: Total Docs (2,400), Stale Docs (487 - 20%), Orphaned Docs (156), Avg Completeness (61%), Quality Trend (line graph showing last 90 days)."
      - "Content table with columns: Title, Status (Stale/Healthy/Orphaned with color badges), Owner, Last Updated (X days/months ago), View Count (90 days), Quality Score (0-100), Actions menu (Reassign, Review, Archive)."
      - "Filter controls: Department dropdown (All/Engineering/Product/HR/etc.), Category tree selector, Status checkboxes (Stale/Healthy/Orphaned), Date range picker (Last Updated)."
      - "Detail view showing: quality flags (Broken Links: 3, Missing Sections: 2, Completeness: 78%), edit history timeline, view analytics graph (views over time), ownership info (current owner, assignment date, auto-review schedule)."
      - "Bulk operation toolbar appearing when 2+ docs selected: 'Reassign ownership to...', 'Set staleness threshold to...', 'Trigger review reminder', 'Move to category...', '# docs selected' counter."

  - id: ctx-003
    type: ui
    name: Knowledge Base Search & Discovery Interface
    description: >
      All employees access this interface when searching for organizational documentation. Provides semantic search with quality-enhanced results,
      related content discovery, and feedback mechanisms for continuous knowledge improvement.
    key_interactions:
      - "Enter search query in search bar with semantic search enabled (finds content by meaning not just keyword matching via fd-001 integration)."
      - "Review search results with quality indicators (status badges, freshness, authority signals, view counts, quality scores) to identify most relevant authoritative content."
      - "Click result to open document with currency confidence indicators (last reviewed, next review, version, edit history) enabling trust assessment."
      - "Discover related content via sidebar panels: Related by Category, Related by Tags, Referenced By, Frequently Viewed Together enabling serendipitous knowledge connections."
      - "Provide feedback on content via 'Provide Feedback' button opening inline comment anchored to specific section for targeted improvement suggestions."
    data_displayed:
      - "Search results list showing for each result: Title with highlighting, Status badge (Published/Under Review/Archived with color), Freshness (Updated X ago), Authority (Official - Team or Draft - Owner), View count (X views this month), Quality score (0-100), 2-sentence preview."
      - "Document header showing currency indicators: Last Reviewed (12 days ago by Security Team), Next Review (February 2024), Version (2.3 with hover showing change summary: 'Major update Nov 2023 for PCI DSS v4.0'), Edit History link."
      - "Related Content sidebar with 4 collapsible sections: Related by Category (5 docs with titles + view counts), Related by Tags (docs sharing 2+ tags), Referenced By (4 docs linking here), Frequently Viewed Together (3 docs with 60%+ overlap + view percentages)."
      - "Feedback interface showing: anchor location (section name), text input (50-500 chars), urgency selector (Low/Medium/High), submit button, existing feedback count (3 comments) with view/hide toggle."
      - "Notification toast when feedback addressed: '{Owner} responded to your feedback on {Doc}' with 'View Response' link and timestamp."
